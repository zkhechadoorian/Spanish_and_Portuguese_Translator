{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "# Neural machine translation with attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g7GssFmcH4M"
      },
      "source": [
        "## Overview 😊"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmL8nXwXcH4N"
      },
      "source": [
        "### Overall Structure of This Notebook 📝✨\n",
        "\n",
        "1. **Introduction** 🚀\n",
        "    - Overview of neural machine translation with attention\n",
        "    - Motivation and goals\n",
        "\n",
        "2. **Setup** ⚙️\n",
        "    - Installation of required packages\n",
        "    - Importing libraries\n",
        "\n",
        "3. **Data Preparation** 🗂️\n",
        "    - Download and inspect the Spanish-English dataset\n",
        "    - Load and split data into context (Spanish) and target (English)\n",
        "    - Create `tf.data.Dataset` objects for training and validation\n",
        "\n",
        "4. **Text Preprocessing** 🧹🔤\n",
        "    - Standardize and clean sentences\n",
        "    - Tokenize and vectorize text using Keras `TextVectorization`\n",
        "    - Convert sentences to padded token ID tensors\n",
        "\n",
        "5. **Model Components** 🧩\n",
        "    - Encoder: Bidirectional GRU for context processing\n",
        "    - Attention Layer: Cross-attention mechanism\n",
        "    - Decoder: Unidirectional GRU for target sequence generation\n",
        "\n",
        "6. **Training Preparation** 🏋️‍♂️\n",
        "    - Masking and handling padding tokens\n",
        "    - Custom loss and accuracy functions\n",
        "\n",
        "7. **Model Construction** 🏗️\n",
        "    - Combine encoder, attention, and decoder into a `Translator` model\n",
        "\n",
        "8. **Training** 🔥\n",
        "    - Compile and train the model\n",
        "    - Monitor loss and accuracy\n",
        "\n",
        "9. **Evaluation and Visualization** 📊👀\n",
        "    - Plot training/validation loss and accuracy\n",
        "    - Visualize attention weights\n",
        "\n",
        "10. **Inference and Translation** 🌍💬\n",
        "     - Translate new sentences\n",
        "     - Export and use the trained model\n",
        "\n",
        "11. **Conclusion** ✅\n",
        "     - Summary and next steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh8WNEwYA3BW"
      },
      "source": [
        "This tutorial demonstrates how to train a sequence-to-sequence (seq2seq) model for Spanish-to-English translation roughly based on [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025v5) (Luong et al., 2015).\n",
        "\n",
        "\n",
        "This tutorial: An encoder/decoder connected by attention.\n",
        "\n",
        "\n",
        "While this architecture is somewhat outdated, it is still a very useful project to work through to get a deeper understanding of sequence-to-sequence models and attention mechanisms (before going on to [Transformers](transformer.ipynb))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiwtNgENbx2g"
      },
      "source": [
        "\n",
        "\n",
        "This example assumes some knowledge of TensorFlow fundamentals below the level of a Keras layer:\n",
        "  * [Working with tensors](https://www.tensorflow.org/guide/tensor) directly\n",
        "  * [Writing custom `keras.Model`s and `keras.layers`](https://www.tensorflow.org/guide/keras/custom_layers_and_models)\n",
        "\n",
        "After training the model in this notebook, you will be able to input a Spanish sentence, such as \"*¿todavia estan en casa?*\", and return the English translation: \"*are you still at home?*\"\n",
        "\n",
        "The resulting model is exportable as a `tf.saved_model`, so it can be used in other TensorFlow environments.\n",
        "\n",
        "The translation quality is reasonable for a toy example, but the generated attention plot is perhaps more interesting. This shows which parts of the input sentence has the model's attention while translating:\n",
        "\n",
        "<img src=\"https://tensorflow.org/images/spanish-english.png\" alt=\"spanish-english attention plot\">\n",
        "\n",
        "Note: This example takes approximately 10 minutes to run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAmSR1FaqKrl"
      },
      "source": [
        "## Setup ⚙️"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-O9U-B9XcH4O"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.19.0 tensorflow-text matplotlib einops wrapt==1.15.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpumA1eYNR7G"
      },
      "outputs": [],
      "source": [
        "# !pip install typing_extensions==4.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjREleprM_Wm"
      },
      "outputs": [],
      "source": [
        "!export WRAPT_DISABLE_EXTENSIONS=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnxXKDjq3jEL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "import wrapt\n",
        "if wrapt.__version__!=\"1.15.0\" or tf.__version__!=\"2.19.0\":\n",
        "  raise Exception(f\"Please restart your session as you are still using warpt version: {wrapt.__version__}\")\n",
        "gpus = tf.config.list_physical_devices()\n",
        "if len(gpus)>1:\n",
        "  raise Exception(\"Please use CPUs for this notebook only. Go to Runtime at the top, then press change runtime to CPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_yq8kvIqoqQ"
      },
      "source": [
        "This tutorial uses a lot of low level API's where it's easy to get shapes wrong. This class is used to check shapes throughout the tutorial. 🧩\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqFqKi4fqN9X"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "class ShapeChecker():\n",
        "  def __init__(self):\n",
        "    # Keep a cache of every axis-name seen\n",
        "    self.shapes = {}\n",
        "\n",
        "  def __call__(self, tensor, names, broadcast=False):\n",
        "    # Only check shapes when running eagerly (not in graph mode)\n",
        "    if not tf.executing_eagerly():\n",
        "      return\n",
        "\n",
        "    # einops is a Python library for flexible and readable tensor operations.\n",
        "    # It provides functions to parse, reshape, and manipulate tensor shapes using simple string-based notation.\n",
        "    # Here, einops.parse_shape is used to extract the dimensions of the tensor according to the provided axis names.\n",
        "    parsed = einops.parse_shape(tensor, names)\n",
        "\n",
        "    # Check each axis name and its dimension\n",
        "    for name, new_dim in parsed.items():\n",
        "      old_dim = self.shapes.get(name, None)\n",
        "\n",
        "      # If broadcasting is allowed and the new dimension is 1, skip the check\n",
        "      if (broadcast and new_dim == 1):\n",
        "        continue\n",
        "\n",
        "      if old_dim is None:\n",
        "        # If the axis name is new, add its length to the cache.\n",
        "        self.shapes[name] = new_dim\n",
        "        continue\n",
        "\n",
        "      # If the dimension has changed, raise an error\n",
        "      if new_dim != old_dim:\n",
        "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
        "                         f\"    found: {new_dim}\\n\"\n",
        "                         f\"    expected: {old_dim}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjUROhJfH3ML"
      },
      "source": [
        "## Data Preparation 🗂️"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puE_K74DIE9W"
      },
      "source": [
        "The tutorial uses a language dataset provided by [Anki](http://www.manythings.org/anki/). This dataset contains language translation pairs in the format:\n",
        "\n",
        "```\n",
        "May I borrow this book?\t¿Puedo tomar prestado este libro?\n",
        "```\n",
        "\n",
        "They have a variety of languages available, but this example uses the English-Spanish dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfodePkj3jEa"
      },
      "source": [
        "### Download and prepare the dataset 📥🗂️\n",
        "\n",
        "For convenience, a copy of this dataset is hosted on Google Cloud, but you can also download your own copy. After downloading the dataset, here are the steps you need to take to prepare the data:\n",
        "\n",
        "1. Add a *start* and *end* token to each sentence. 🚩🏁\n",
        "2. Clean the sentences by removing special characters. 🧹\n",
        "3. Create a word index and reverse word index (dictionaries mapping from word → id and id → word). 🔢🔄\n",
        "4. Pad each sentence to a maximum length. 📏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRVATYOgJs1b"
      },
      "outputs": [],
      "source": [
        "# Download the Spanish-English translation dataset as a zip file.\n",
        "import pathlib\n",
        "\n",
        "# Use TensorFlow utility to download and extract the dataset.\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip',  # Name of the file to download.\n",
        "    origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',  # URL of the dataset.\n",
        "    extract=True  # Automatically extract the zip file after downloading.\n",
        ")\n",
        "\n",
        "# Construct the path to the extracted text file containing sentence pairs.\n",
        "path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng_extracted/spa-eng/spa.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onQYEMHqcH4Q"
      },
      "source": [
        "To take a look of the data, we will call the cat command and display the first 10 lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f55wtYYVcH4Q"
      },
      "outputs": [],
      "source": [
        "!cat {path_to_file} | head -n 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHn4Dct23jEm"
      },
      "outputs": [],
      "source": [
        "def load_data(path:pathlib.Path):\n",
        "  \"\"\"This function purpose is to load the dataset from a text file and split it into target and context, which means having a word and it's corresponding translation.\"\"\"\n",
        "  # Read the entire text file as a string using UTF-8 encoding\n",
        "  text = path.read_text(encoding='utf-8')\n",
        "\n",
        "  # Split the text into lines\n",
        "  lines = text.splitlines()\n",
        "  # Split each line into a pair (target, context) using tab as the separator\n",
        "  pairs = [line.split('\\t') for line in lines]\n",
        "\n",
        "  # Extract the context sentences (Spanish) from each pair\n",
        "  context = np.array([context for target, context in pairs])\n",
        "  # Extract the target sentences (English) from each pair\n",
        "  target = np.array([target for target, context in pairs])\n",
        "\n",
        "  # Return the target and context arrays\n",
        "  return target, context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTbSbBz55QtF"
      },
      "outputs": [],
      "source": [
        "# Load the data from the specified file path.\n",
        "# The load_data function reads the file, splits each line into English (target) and Spanish (context) sentence pairs,\n",
        "# and returns them as numpy arrays.\n",
        "target_raw, context_raw = load_data(path_to_file)\n",
        "\n",
        "print(context_raw[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lH_dPY8TRp3c"
      },
      "outputs": [],
      "source": [
        "print(target_raw[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgCLkfv5uO3d"
      },
      "source": [
        "### Create a tf.data dataset 📦"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfVWx3WaI5Df"
      },
      "source": [
        "From these arrays of strings you can create a `tf.data.Dataset` of strings that shuffles and batches them efficiently: 🎲📦"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rZFgz69nMPa"
      },
      "outputs": [],
      "source": [
        "# Set the buffer size for shuffling to the total number of context sentences\n",
        "BUFFER_SIZE = len(context_raw)\n",
        "# Set the batch size for training and validation\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Randomly assign each example to the training set (80%) or validation set (20%)\n",
        "is_train = np.random.uniform(size=(len(target_raw),)) < 0.8\n",
        "\n",
        "# Create the training dataset:\n",
        "# - Select context and target sentences assigned to training\n",
        "# - Shuffle the dataset\n",
        "# - Batch the dataset\n",
        "train_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((context_raw[is_train], target_raw[is_train]))\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE))\n",
        "\n",
        "# Create the validation dataset:\n",
        "# - Select context and target sentences not assigned to training\n",
        "# - Shuffle the dataset\n",
        "# - Batch the dataset\n",
        "val_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((context_raw[~is_train], target_raw[~is_train]))\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qc6-NK1GtWQt"
      },
      "outputs": [],
      "source": [
        "# Iterate over one batch from the training dataset\n",
        "for example_context_strings, example_target_strings in train_raw.take(1):\n",
        "  # Print the first 5 context (Spanish) sentences in the batch\n",
        "  print(example_context_strings[:5])\n",
        "  print()\n",
        "  # Print the first 5 target (English) sentences in the batch\n",
        "  print(example_target_strings[:5])\n",
        "  break  # Exit after processing the first batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCoxLcuN3bwv"
      },
      "source": [
        "### Text preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kwdPcHvzz_a"
      },
      "source": [
        "One of the goals of this tutorial is to build a model that can be exported as a `tf.saved_model`. To make that exported model useful it should take `tf.string` inputs, and return `tf.string` outputs: All the text processing happens inside the model. Mainly using a `layers.TextVectorization` layer. 🏗️🔤✨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOQ5n55X4uDB"
      },
      "source": [
        "#### Standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upKhKAMK4zzI"
      },
      "source": [
        "The model is dealing with multilingual text with a limited vocabulary. So it will be important to standardize the input text. 🌐🔤\n",
        "\n",
        "The first step is Unicode normalization to split accented characters and replace compatibility characters with their ASCII equivalents. 🧹\n",
        "\n",
        "The `tensorflow_text` package contains a unicode normalize operation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mD0e-DWGQ2Vo"
      },
      "outputs": [],
      "source": [
        "# Define a sample Spanish sentence as a TensorFlow constant\n",
        "example_text = tf.constant('¿Todavía está en casa?')\n",
        "\n",
        "# Print the raw bytes of the example text\n",
        "print(example_text.numpy())\n",
        "\n",
        "# Normalize the example text using Unicode normalization (NFKD form)\n",
        "# This splits accented characters and replaces compatibility characters with their ASCII equivalents\n",
        "print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hTllEjK6RSo"
      },
      "source": [
        "Unicode normalization will be the first step in the text standardization function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chTF5N885F0P"
      },
      "outputs": [],
      "source": [
        "def tf_lower_and_split_punct(text):\n",
        "  # Unicode normalization (NFKD) splits accented characters into base + accent,\n",
        "  # and replaces compatibility characters with their canonical equivalents.\n",
        "  # This helps standardize multilingual text for processing.\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "\n",
        "  # Convert all characters to lowercase for consistency.\n",
        "  text = tf.strings.lower(text)\n",
        "\n",
        "  # Remove all characters except spaces, lowercase letters, and select punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "\n",
        "  # Add spaces around punctuation marks to separate them from words.\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "\n",
        "  # Remove leading and trailing whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  # Add [START] and [END] tokens to mark the beginning and end of the sentence.\n",
        "  # These tokens help the model know where a sentence starts and ends,\n",
        "  # which is important for sequence-to-sequence tasks like translation.\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UREvDg3sEKYa"
      },
      "outputs": [],
      "source": [
        "# Print the original example text as a string\n",
        "print(example_text.numpy().decode())\n",
        "\n",
        "# Print the example text after applying the text preprocessing function\n",
        "# This function normalizes, lowercases, removes unwanted characters,\n",
        "# adds spaces around punctuation, strips whitespace, and adds [START] and [END] tokens\n",
        "print(tf_lower_and_split_punct(example_text).numpy().decode())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q-sKsSI7xRZ"
      },
      "source": [
        "#### Text Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aKn8qd37abi"
      },
      "source": [
        "This standardization function will be wrapped up in a `tf.keras.layers.TextVectorization` layer 😊 which will handle the vocabulary extraction and conversion of input text to sequences of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAY9k49G3jE_"
      },
      "outputs": [],
      "source": [
        "# Set the maximum vocabulary size for the text vectorization layer\n",
        "max_vocab_size = 5000\n",
        "\n",
        "# Create a TextVectorization layer for processing Spanish context sentences.\n",
        "# - standardize: applies the tf_lower_and_split_punct function to clean and tokenize the text\n",
        "# - max_tokens: limits the vocabulary size to max_vocab_size\n",
        "# - ragged: allows variable-length outputs for tokenized sentences\n",
        "context_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size,\n",
        "    ragged=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kbC6ODP8IK_"
      },
      "source": [
        "The `TextVectorization` layer and many other [Keras preprocessing layers](https://www.tensorflow.org/guide/keras/preprocessing_layers) have an `adapt` method. This method reads one epoch of the training data, and works a lot like `Model.fit`. This `adapt` method initializes the layer based on the data. Here it determines the vocabulary: 🧠🔤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmsI1Yql8FYe"
      },
      "outputs": [],
      "source": [
        "# Adapt the context_text_processor to the training data.\n",
        "# This step analyzes the context (Spanish) sentences in train_raw,\n",
        "# builds the vocabulary, and prepares the text vectorization layer.\n",
        "context_text_processor.adapt(train_raw.map(lambda context, target: context))\n",
        "\n",
        "# Display the first 10 words from the vocabulary learned by the context_text_processor.\n",
        "context_text_processor.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kGjIFjX8_Wp"
      },
      "source": [
        "That's the Spanish `TextVectorization` layer, now build and `.adapt()` the English one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlC4xuZnKLBS"
      },
      "outputs": [],
      "source": [
        "# Create a TextVectorization layer for processing English target sentences.\n",
        "# - standardize: applies the tf_lower_and_split_punct function to clean and tokenize the text\n",
        "# - max_tokens: limits the vocabulary size to max_vocab_size\n",
        "# - ragged: allows variable-length outputs for tokenized sentences\n",
        "target_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size,\n",
        "    ragged=True)\n",
        "\n",
        "# Adapt the target_text_processor to the training data.\n",
        "# This step analyzes the target (English) sentences in train_raw,\n",
        "# builds the vocabulary, and prepares the text vectorization layer.\n",
        "target_text_processor.adapt(train_raw.map(lambda context, target: target))\n",
        "\n",
        "# Display the first 10 words from the vocabulary learned by the target_text_processor.\n",
        "target_text_processor.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWQqlP_s9eIv"
      },
      "source": [
        "Now these layers can convert a batch of strings into a batch of token IDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KZxj8IrNZ9S"
      },
      "outputs": [],
      "source": [
        "# Convert the batch of example context strings (Spanish sentences) into token IDs using the context_text_processor\n",
        "example_tokens = context_text_processor(example_context_strings)\n",
        "\n",
        "# Display the token IDs for the first 3 sentences in the batch\n",
        "example_tokens[:3, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA9rUn9G9n78"
      },
      "source": [
        "The `get_vocabulary` method can be used to convert token IDs back to text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98g9rcxGQY0I"
      },
      "outputs": [],
      "source": [
        "# Get the vocabulary from the context_text_processor as a numpy array\n",
        "context_vocab = np.array(context_text_processor.get_vocabulary())\n",
        "\n",
        "# Convert the first example's token IDs to their corresponding words using the vocabulary\n",
        "tokens = context_vocab[example_tokens[0].numpy()]\n",
        "\n",
        "# Join the tokens into a single string for readability\n",
        "' '.join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot0aCL9t-Ghi"
      },
      "source": [
        "The returned token IDs are zero-padded. This can easily be turned into a mask:\n",
        "\n",
        "---\n",
        "\n",
        "### Why masking is important 🛡️\n",
        "\n",
        "Token sequences are padded with zeros to ensure all sequences in a batch have the same length. These padding tokens do **not** represent actual data and should be ignored during computations such as loss calculation, accuracy measurement, and attention visualization.\n",
        "\n",
        "Without masking, the model would treat padding as meaningful input, introducing noise and bias that can negatively affect training and evaluation. The mask allows us to focus only on the real tokens, ensuring that metrics and model updates are based solely on valid data. ✅\n",
        "\n",
        "**In summary:**  \n",
        "- Padding tokens = 🚫 not real data  \n",
        "- Masking = 🕵️‍♂️ focus on valid tokens  \n",
        "- Better training & evaluation = 🎯"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jx4Or_eFRSz"
      },
      "outputs": [],
      "source": [
        "# If running in a Jupyter notebook, ensure plots display inline\n",
        "%matplotlib inline\n",
        "\n",
        "# Plot the token IDs for the batch of example sentences.\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(example_tokens.to_tensor())  # Convert ragged tensor to dense and plot token IDs\n",
        "plt.title('Token IDs')\n",
        "\n",
        "# Plot the mask showing which positions are non-padding (token ID != 0)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(example_tokens.to_tensor() != 0)  # True for non-padding tokens\n",
        "plt.title('Mask')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O0B4XdFlRgc"
      },
      "source": [
        "### Process the dataset 🛠️✨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVCuyuSp_whd"
      },
      "source": [
        "The `process_text` function below converts the `Datasets` of strings, into  0-padded tensors of token IDs. It also converts from a `(context, target)` pair to an `((context, target_in), target_out)` pair for training with `keras.Model.fit`. Keras expects `(inputs, labels)` pairs, the inputs are the `(context, target_in)` and the labels are `target_out`. The difference between `target_in` and `target_out` is that they are shifted by one step relative to eachother, so that at each location the label is the next token. 🧩🔢"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk5tbZWQl5u1"
      },
      "outputs": [],
      "source": [
        "def process_text(context, target):\n",
        "  # Convert context sentences (Spanish) to token IDs and pad to tensor\n",
        "  context = context_text_processor(context).to_tensor()\n",
        "  # Convert target sentences (English) to token IDs (ragged tensor)\n",
        "  target = target_text_processor(target)\n",
        "  # Prepare decoder input by removing the last token ([END])\n",
        "  targ_in = target[:, :-1].to_tensor()\n",
        "  # Prepare decoder output by removing the first token ([START])\n",
        "  targ_out = target[:, 1:].to_tensor()\n",
        "  # Return ((context, decoder_input), decoder_output) for training\n",
        "  return (context, targ_in), targ_out\n",
        "\n",
        "# Map the process_text function over the training and validation datasets\n",
        "train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
        "val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iGi7X2m_tbM"
      },
      "source": [
        "Here is the first sequence of each, from the first batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woQBWAjLsJkr"
      },
      "outputs": [],
      "source": [
        "# Iterate over one batch from the training dataset\n",
        "for (ex_context_tok, ex_tar_in), ex_tar_out in train_ds.take(1):\n",
        "  # Print the first 10 token IDs of the first context (Spanish) sentence in the batch\n",
        "  print(ex_context_tok[0, :10].numpy())\n",
        "  print()\n",
        "  # Print the first 10 token IDs of the first target (English) input sentence in the batch\n",
        "  print(ex_tar_in[0, :10].numpy())\n",
        "  # Print the first 10 token IDs of the first target (English) output sentence in the batch\n",
        "  print(ex_tar_out[0, :10].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNfHIF71ulLu"
      },
      "source": [
        "## The encoder/decoder 🤖📝\n",
        "\n",
        "The following diagrams shows an overview of the model. In both the encoder is on the left, the decoder is on the right. At each time-step the decoder's output is combined with the encoder's output, to predict the next word.\n",
        "\n",
        "The original [left] contains a few extra connections that are intentionally omitted from this tutorial's model [right], as they are generally unnecessary, and difficult to implement. Those missing connections are:\n",
        "\n",
        "1. Feeding the state from the encoder's RNN to the decoder's RNN\n",
        "2. Feeding the attention output back to the RNN's input.\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=500 src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\"/>\n",
        "  </td>\n",
        "  <td>\n",
        "   <img width=380 src=\"https://www.tensorflow.org/images/tutorials/transformer/RNN+attention.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th colspan=1>The original from <a href=https://arxiv.org/abs/1508.04025v5>Effective Approaches to Attention-based Neural Machine Translation</a></th>\n",
        "  <th colspan=1>This tutorial's model</th>\n",
        "<tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzQWx2saImMV"
      },
      "source": [
        "Before getting into it define constants for the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a9uNz3-IrF-"
      },
      "outputs": [],
      "source": [
        "# Set the number of units (dimensions) for the model's internal representations.\n",
        "# This value is used for the size of the RNN hidden state and embedding vectors.\n",
        "UNITS = 256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blNgVbLSzpsr"
      },
      "source": [
        "### The encoder 🤖\n",
        "\n",
        "The goal of the encoder is to process the context sequence into a sequence of vectors that are useful for the decoder as it attempts to predict the next output for each timestep. Since the context sequence is constant, there is no restriction on how information can flow in the encoder, so use a bidirectional-RNN to do the processing:\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=500 src=\"https://tensorflow.org/images/tutorials/transformer/RNN-bidirectional.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>A bidirectional RNN</th>\n",
        "<tr>\n",
        "</table>\n",
        "\n",
        "The encoder:\n",
        "\n",
        "1. Takes a list of token IDs (from `context_text_processor`). 🆔\n",
        "2. Looks up an embedding vector for each token (Using a `layers.Embedding`). 🧩\n",
        "3. Processes the embeddings into a new sequence (Using a bidirectional `layers.GRU`). 🔄\n",
        "4. Returns the processed sequence. This will be passed to the attention head. 🎯"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ2rI24i3jFg"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, text_processor, units):\n",
        "    super(Encoder, self).__init__()\n",
        "    # Store the text processor and model units\n",
        "    self.text_processor = text_processor\n",
        "    self.vocab_size = text_processor.vocabulary_size()\n",
        "    self.units = units\n",
        "\n",
        "    # Embedding layer: Converts token IDs to dense vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.vocab_size, units,\n",
        "                                               mask_zero=True)\n",
        "\n",
        "    # Bidirectional GRU: Processes the sequence of embeddings\n",
        "    # Bidirectional GRU: merge_mode='sum' combines the outputs from the forward and backward RNNs by summing them.\n",
        "    # This keeps the output dimension the same as 'units', making it easier to use in downstream layers.\n",
        "    self.rnn = tf.keras.layers.Bidirectional(\n",
        "      merge_mode='sum',\n",
        "      layer=tf.keras.layers.GRU(units,\n",
        "                # Return the full sequence and final state\n",
        "                return_sequences=True,\n",
        "                recurrent_initializer='glorot_uniform'))\n",
        "\n",
        "  def call(self, x):\n",
        "    # Check input shape: (batch, sequence_length)\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(x, 'batch s')\n",
        "\n",
        "    # Convert token IDs to embeddings: (batch, sequence_length, units)\n",
        "    x = self.embedding(x)\n",
        "    shape_checker(x, 'batch s units')\n",
        "\n",
        "    # Process embeddings with bidirectional GRU: (batch, sequence_length, units)\n",
        "    x = self.rnn(x)\n",
        "    shape_checker(x, 'batch s units')\n",
        "\n",
        "    # Return processed sequence for attention\n",
        "    return x\n",
        "\n",
        "  def convert_input(self, texts):\n",
        "    # Convert input texts to tensor\n",
        "    texts = tf.convert_to_tensor(texts)\n",
        "    # If input is a scalar, add batch dimension\n",
        "    if len(texts.shape) == 0:\n",
        "      texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
        "    # Tokenize and pad input texts\n",
        "    context = self.text_processor(texts).to_tensor()\n",
        "    # Encode the tokenized input\n",
        "    context = self(context)\n",
        "    return context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3SKkaQeGn-Q"
      },
      "source": [
        "Try it out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60gSVh05Jl6l"
      },
      "outputs": [],
      "source": [
        "# Create an Encoder instance using the Spanish text processor and model units.\n",
        "encoder = Encoder(context_text_processor, UNITS)\n",
        "\n",
        "# Pass the batch of context token IDs through the encoder to get the encoded sequence.\n",
        "ex_context = encoder(ex_context_tok)\n",
        "\n",
        "# Print the shape of the input context tokens (batch size, sequence length).\n",
        "print(f'Context tokens, shape (batch, s): {ex_context_tok.shape}')\n",
        "\n",
        "# Print the shape of the encoder output (batch size, sequence length, units).\n",
        "print(f'Encoder output, shape (batch, s, units): {ex_context.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45xM_Gl1MgXY"
      },
      "source": [
        "### The attention layer 🧠✨\n",
        "\n",
        "The attention layer lets the decoder access the information extracted by the encoder. It computes a vector from the entire context sequence, and adds that to the decoder's output.\n",
        "\n",
        "The simplest way you could calculate a single vector from the entire sequence would be to take the average across the sequence (`layers.GlobalAveragePooling1D`). An attention layer is similar, but calculates a **weighted** average across the context sequence. Where the weights are calculated from the combination of context and \"query\" vectors.\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=500 src=\"https://www.tensorflow.org/images/tutorials/transformer/CrossAttention-new-full.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th colspan=1>The attention layer</th>\n",
        "<tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ql3ymqwD8LS"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, **kwargs):\n",
        "    super().__init__()\n",
        "    # Multi-head attention layer (single head, key_dim=units)\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n",
        "    # Layer normalization for output stability\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    # Add layer for residual connection\n",
        "    # Residual connections help gradients flow through the network, improve training stability,\n",
        "    # and allow the model to reuse the original input along with the attention output.\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "  def call(self, x, context):\n",
        "    shape_checker = ShapeChecker()\n",
        "\n",
        "    # x: target sequence embeddings (batch, t, units)\n",
        "    shape_checker(x, 'batch t units')\n",
        "    # context: encoder output (batch, s, units)\n",
        "    shape_checker(context, 'batch s units')\n",
        "\n",
        "    # Compute attention output and attention scores\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,         # queries: target sequence\n",
        "        value=context,   # values: context sequence (encoder output)\n",
        "        return_attention_scores=True)\n",
        "\n",
        "    # Check shapes after attention\n",
        "    shape_checker(x, 'batch t units')\n",
        "    shape_checker(attn_scores, 'batch heads t s')\n",
        "\n",
        "    # Average over heads to get (batch, t, s) attention weights\n",
        "    attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
        "    shape_checker(attn_scores, 'batch t s')\n",
        "    # Cache attention weights for later visualization\n",
        "    self.last_attention_weights = attn_scores\n",
        "\n",
        "    # Residual connection: add attention output to input\n",
        "    x = self.add([x, attn_output])\n",
        "    # Normalize output\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    # Return attended and normalized output\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y7hjPkNMmHh"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the CrossAttention layer with the specified number of units\n",
        "attention_layer = CrossAttention(UNITS)\n",
        "\n",
        "# Create an embedding layer for the target (English) tokens\n",
        "embed = tf.keras.layers.Embedding(\n",
        "    target_text_processor.vocabulary_size(),  # Vocabulary size for target language\n",
        "    output_dim=UNITS,                        # Embedding dimension\n",
        "    mask_zero=True                           # Mask padding tokens\n",
        ")\n",
        "\n",
        "# Embed the input target tokens (decoder input)\n",
        "ex_tar_embed = embed(ex_tar_in)\n",
        "\n",
        "# Apply the attention layer: attend to the encoded context using the embedded target tokens\n",
        "result = attention_layer(ex_tar_embed, ex_context)\n",
        "\n",
        "# Print the shapes of the input and output tensors for verification\n",
        "print(f'Context sequence, shape (batch, s, units): {ex_context.shape}')           # Encoder output\n",
        "print(f'Target sequence, shape (batch, t, units): {ex_tar_embed.shape}')          # Embedded target input\n",
        "print(f'Attention result, shape (batch, t, units): {result.shape}')               # Output of attention layer\n",
        "print(f'Attention weights, shape (batch, t, s):    {attention_layer.last_attention_weights.shape}')  # Attention scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx9fUhi3Pmwp"
      },
      "source": [
        "The attention weights will sum to `1` over the context sequence, at each location in the target sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxyR7cmQPn9P"
      },
      "outputs": [],
      "source": [
        "# Sum the attention weights over the context sequence for the first example in the batch.\n",
        "# This checks that the attention weights for each target token sum to 1 (as expected for a probability distribution).\n",
        "attention_layer.last_attention_weights[0].numpy().sum(axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AagyXMH-Jhqt"
      },
      "source": [
        "\n",
        "\n",
        "Here are the attention weights across the context sequences at `t=0`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rqr8XGsAJlf6"
      },
      "outputs": [],
      "source": [
        "# Get the attention weights from the attention layer\n",
        "attention_weights = attention_layer.last_attention_weights\n",
        "\n",
        "# Create a mask for the context tokens (True for non-padding tokens)\n",
        "mask = (ex_context_tok != 0).numpy()\n",
        "\n",
        "# Plot the attention weights for the first target token, masked by valid context positions\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(mask * attention_weights[:, 0, :])\n",
        "plt.title('Attention weights')\n",
        "\n",
        "# Plot the mask itself to show valid context positions\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(mask)\n",
        "plt.title('Mask');\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Eil-C_NN1rp"
      },
      "source": [
        "🔍 **Interpreting Initial Attention Weights**\n",
        "\n",
        "Because of the small-random initialization, the attention weights are initially all close to `1/(sequence_length)`.  \n",
        "This means that, before training, the model does not \"know\" which parts of the input are important, so it distributes its attention almost uniformly across all positions in the context sequence.\n",
        "\n",
        "As training progresses, the model learns to assign higher attention weights to the most relevant tokens in the input sequence for each output token.  \n",
        "This non-uniform attention allows the model to focus on specific words or phrases that are most helpful for generating accurate translations, improving both the quality of the output and the interpretability of the attention mechanism. 🎯✨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ638eHN4iCK"
      },
      "source": [
        "### The decoder 🤖📝\n",
        "\n",
        "The decoder's job is to generate predictions for the next token at each location in the target sequence.\n",
        "\n",
        "1. It looks up embeddings for each token in the target sequence. 🧩\n",
        "2. It uses an RNN to process the target sequence, and keep track of what it has generated so far. 🔄\n",
        "3. It uses RNN output as the \"query\" to the attention layer, when attending to the encoder's output. 🎯\n",
        "4. At each location in the output it predicts the next token. 📝\n",
        "\n",
        "When training, the model predicts the next word at each location. So it's important that the information only flows in one direction through the model. The decoder uses a unidirectional (not bidirectional) RNN to process the target sequence.\n",
        "\n",
        "When running inference with this model it produces one word at a time, and those are fed back into the model.\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=500 src=\"https://tensorflow.org/images/tutorials/transformer/RNN.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>A unidirectional RNN</th>\n",
        "<tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZsQJMqNmg_L"
      },
      "source": [
        "Here is the `Decoder` class' initializer. The initializer creates all the necessary layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erYvHIgAl8kh"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    # Utility to add methods to the class dynamically (will be used after class object creation and dynamically add methods)\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, text_processor, units):\n",
        "      super(Decoder, self).__init__()\n",
        "      # Store the text processor and model units\n",
        "      self.text_processor = text_processor\n",
        "      self.vocab_size = text_processor.vocabulary_size()\n",
        "\n",
        "      # Lookup layer: maps words to token IDs\n",
        "      self.word_to_id = tf.keras.layers.StringLookup(\n",
        "        vocabulary=text_processor.get_vocabulary(),\n",
        "        mask_token='', oov_token='[UNK]')\n",
        "\n",
        "      # Lookup layer: maps token IDs back to words\n",
        "      self.id_to_word = tf.keras.layers.StringLookup(\n",
        "        vocabulary=text_processor.get_vocabulary(),\n",
        "        mask_token='', oov_token='[UNK]',\n",
        "        invert=True)\n",
        "\n",
        "      # Special token IDs for start and end of sequence\n",
        "      self.start_token = self.word_to_id('[START]')\n",
        "      self.end_token = self.word_to_id('[END]')\n",
        "\n",
        "      self.units = units\n",
        "\n",
        "      # 1. Embedding layer: converts token IDs to dense vectors\n",
        "      self.embedding = tf.keras.layers.Embedding(\n",
        "        self.vocab_size, units, mask_zero=True)\n",
        "\n",
        "      # 2. RNN layer: processes the sequence and keeps track of generated tokens\n",
        "      self.rnn = tf.keras.layers.GRU(\n",
        "        units,\n",
        "        return_sequences=True,\n",
        "        return_state=True,\n",
        "        recurrent_initializer='glorot_uniform'\n",
        "      )\n",
        "\n",
        "      # 3. Attention layer: attends to the encoder output using RNN output as query\n",
        "      self.attention = CrossAttention(units)\n",
        "\n",
        "      # 4. Output layer: produces logits for each output token\n",
        "      self.output_layer = tf.keras.layers.Dense(self.vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd8-nRNzFR8x"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPnaw583CpnY"
      },
      "source": [
        "Next, the `call` method, takes 3 arguments: 🤖\n",
        "\n",
        "* `inputs` -  a `context, x` pair where:\n",
        "  * `context` - is the context from the encoder's output. 🌐\n",
        "  * `x` - is the target sequence input. 📝\n",
        "* `state` - Optional, the previous `state` output from the decoder (the internal state of the decoder's RNN). Pass the state from a previous run to continue generating text where you left off. 🔄\n",
        "* `return_state` - [Default: False] - Set this to `True` to return the RNN state. 🏁"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJOi5btHAPNK"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def call(self,\n",
        "  context, x,\n",
        "  state=None,\n",
        "  return_state=False):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(x, 'batch t')  # Check shape of target input tokens\n",
        "    shape_checker(context, 'batch s units')  # Check shape of encoder output\n",
        "\n",
        "    # 1. Lookup the embeddings for the target input tokens\n",
        "    x = self.embedding(x)\n",
        "    shape_checker(x, 'batch t units')  # Check shape after embedding\n",
        "\n",
        "    # 2. Process the target sequence with the RNN\n",
        "    x,state=self.rnn(x, initial_state=state)\n",
        "    shape_checker(x, 'batch t units')  # Check shape after RNN\n",
        "\n",
        "    # 3. Use the RNN output as the query for attention over the context\n",
        "    x = self.attention(x, context)\n",
        "    self.last_attention_weights = self.attention.last_attention_weights  # Cache attention weights for visualization\n",
        "    shape_checker(x, 'batch t units')  # Check shape after attention\n",
        "    shape_checker(self.last_attention_weights, 'batch t s')  # Check shape of attention weights\n",
        "\n",
        "    # 4. Generate logit predictions for the next token\n",
        "    logits = self.output_layer(x)\n",
        "    shape_checker(logits, 'batch t target_vocab_size')  # Check shape of output logits\n",
        "\n",
        "    # Optionally return the RNN state for inference\n",
        "    if return_state:\n",
        "     return logits, state\n",
        "    else:\n",
        "     return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1-mLAcUEXpK"
      },
      "source": [
        "That will be sufficient for training. Create an instance of the decoder to test out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZUMbYXIEVeA"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the Decoder class using the target_text_processor and UNITS.\n",
        "# The Decoder will be used to generate English translations from encoded Spanish context.\n",
        "decoder = Decoder(target_text_processor, UNITS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFWaI4wqzt4t"
      },
      "source": [
        "In training you'll use the decoder like this:\n",
        "\n",
        "Given the context and target tokens, for each target token it predicts the next target token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YM-lD7bzx18"
      },
      "outputs": [],
      "source": [
        "# Print the shapes of the encoder output, input target tokens, and logits for inspection\n",
        "logits = decoder(ex_context, ex_tar_in)\n",
        "\n",
        "# The encoder output shape: (batch, sequence_length, units)\n",
        "print(f'encoder output shape: (batch, s, units) {ex_context.shape}')\n",
        "\n",
        "# The input target tokens shape: (batch, target_sequence_length)\n",
        "print(f'input target tokens shape: (batch, t) {ex_tar_in.shape}')\n",
        "\n",
        "# The logits shape: (batch, target_sequence_length, target_vocabulary_size)\n",
        "print(f'logits shape shape: (batch, target_vocabulary_size) {logits.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhS_tbk7VQkX"
      },
      "source": [
        "#### Inference\n",
        "\n",
        "To use it for inference you'll need a couple more methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPm12cnIVRQr"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def get_initial_state(self, context):\n",
        "  # This method is needed for inference to initialize the decoder's generation loop.\n",
        "  # It provides the initial start tokens, a mask indicating which sequences are finished,\n",
        "  # and the initial RNN state for the decoder. This setup allows the model to begin\n",
        "  # generating output tokens one step at a time, starting from the [START] token.\n",
        "\n",
        "  batch_size = tf.shape(context)[0]\n",
        "  start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "  embedded = self.embedding(start_tokens)\n",
        "  batch_size = tf.shape(embedded)[0]\n",
        "  return start_tokens, done, self.rnn.get_initial_state(batch_size)[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzeOhpBvVS5L"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def tokens_to_text(self, tokens):\n",
        "  # During inference, the model generates sequences of token IDs.\n",
        "  # This method converts those token IDs back to human-readable text.\n",
        "  # It's needed in inference to interpret the model's output as actual sentences.\n",
        "\n",
        "  # Convert token IDs to words using the id_to_word lookup layer\n",
        "  words = self.id_to_word(tokens)\n",
        "  # Join the words into sentences (strings), separating by spaces\n",
        "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "  # Remove the [START] token from the beginning of each sentence\n",
        "  result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
        "  # Remove the [END] token from the end of each sentence\n",
        "  result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
        "  # Return the cleaned sentences as strings\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6ildnz_V1MA"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n",
        "  # This method is needed for inference because, during generation,\n",
        "  # we produce one token at a time, feeding the previous output back in.\n",
        "  # It runs the decoder for a single step, predicts the next token,\n",
        "  # and updates the RNN state and done mask for each sequence in the batch.\n",
        "\n",
        "  # Run the decoder for one step to get logits and new RNN state\n",
        "  logits, state = self(\n",
        "    context, next_token,\n",
        "    state = state,\n",
        "    return_state=True)\n",
        "\n",
        "  # If temperature is 0, use greedy decoding (argmax)\n",
        "  if temperature == 0.0:\n",
        "    next_token = tf.argmax(logits, axis=-1)\n",
        "  else:\n",
        "    # Otherwise, sample from the probability distribution (softmax with temperature)\n",
        "    logits = logits[:, -1, :]/temperature\n",
        "    next_token = tf.random.categorical(logits, num_samples=1)\n",
        "\n",
        "  # Mark sequences as done if they produce the end token\n",
        "  done = done | (next_token == self.end_token)\n",
        "  # For finished sequences, pad with zeros\n",
        "  next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
        "\n",
        "  # Return the next token, done mask, and new state\n",
        "  return next_token, done, state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WiXLrVs-FTE"
      },
      "source": [
        "With those extra functions, you can write a generation loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuehagxL-JBZ"
      },
      "outputs": [],
      "source": [
        "# Setup the loop variables for inference.\n",
        "# Get initial decoder state, start tokens, and done mask for the batch.\n",
        "next_token, done, state = decoder.get_initial_state(ex_context)\n",
        "tokens = []\n",
        "\n",
        "# Generate up to 10 tokens for each sequence in the batch.\n",
        "for n in range(10):\n",
        "  # Run one decoding step: get next token, update done mask and RNN state.\n",
        "  next_token, done, state = decoder.get_next_token(\n",
        "      ex_context, next_token, done, state, temperature=1.0)\n",
        "  # Collect the generated token for this step.\n",
        "  tokens.append(next_token)\n",
        "\n",
        "# Concatenate all generated tokens along the sequence axis to form output sequences.\n",
        "tokens = tf.concat(tokens, axis=-1) # (batch, t)\n",
        "\n",
        "# Convert the token IDs back to text strings.\n",
        "result = decoder.tokens_to_text(tokens)\n",
        "# Display the first 3 generated sequences.\n",
        "result[:3].numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ALTdqCMLGSY"
      },
      "source": [
        "Since the model's untrained, it outputs items from the vocabulary almost uniformly at random."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6xyru86m914"
      },
      "source": [
        "## The model 🤖✨\n",
        "\n",
        "Now that you have all the model components, combine them to build the model for training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWIyuy71TkJT"
      },
      "outputs": [],
      "source": [
        "class Translator(tf.keras.Model):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    # Utility to add methods to the class dynamically\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, units,\n",
        "               context_text_processor,\n",
        "               target_text_processor):\n",
        "    super().__init__()\n",
        "    # Build the encoder and decoder using the provided text processors and units\n",
        "    encoder = Encoder(context_text_processor, units)\n",
        "    decoder = Decoder(target_text_processor, units)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Unpack the inputs: context (source language) and x (target language input)\n",
        "    context, x = inputs\n",
        "    # Encode the context sequence\n",
        "    context = self.encoder(context)\n",
        "    # Decode the target input sequence using the encoded context\n",
        "    logits = self.decoder(context, x)\n",
        "\n",
        "    try:\n",
        "      # Delete the keras mask, so keras doesn't scale the loss+accuracy.\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    # Return the output logits (predictions for next token)\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rPi0FkS2iA5"
      },
      "source": [
        "During training the model will be used like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vhjTh84K6Mg"
      },
      "outputs": [],
      "source": [
        "# Instantiate the Translator model with the specified number of units and text processors\n",
        "model = Translator(UNITS, context_text_processor, target_text_processor)\n",
        "\n",
        "# Pass a batch of context and target input tokens through the model to get output logits\n",
        "logits = model((ex_context_tok, ex_tar_in))\n",
        "\n",
        "# Print the shapes of the input and output tensors for inspection\n",
        "print(f'Context tokens, shape: (batch, s, units) {ex_context_tok.shape}')  # Shape of context input tokens\n",
        "print(f'Target tokens, shape: (batch, t) {ex_tar_in.shape}')              # Shape of target input tokens\n",
        "print(f'logits, shape: (batch, t, target_vocabulary_size) {logits.shape}') # Shape of model output logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "### Training 🏋️‍♂️✨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FmzjGmprVmE"
      },
      "source": [
        "For training, you'll want to implement your own masked loss and accuracy functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmTHr5iV3jFr"
      },
      "outputs": [],
      "source": [
        "def masked_loss(y_true, y_pred):\n",
        "    # Create a loss function for sparse categorical crossentropy.\n",
        "    # from_logits=True means y_pred are raw logits, not probabilities.\n",
        "    # reduction='none' means the loss is computed per element, not averaged.\n",
        "    # SparseCategoricalCrossentropy is needed because the targets (y_true) are integer class labels (token IDs),\n",
        "    # not one-hot encoded vectors. It efficiently computes the cross-entropy loss for each token prediction.\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "\n",
        "    # Compute the loss for each item in the batch.\n",
        "    loss = loss_fn(y_true, y_pred)\n",
        "\n",
        "    # Create a mask to ignore losses for padding tokens (where y_true == 0).\n",
        "    mask = tf.cast(y_true != 0, loss.dtype)\n",
        "    # Apply the mask to the loss.\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the mean loss over all non-padding tokens.\n",
        "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRB1CTmQWOIL"
      },
      "outputs": [],
      "source": [
        "def masked_acc(y_true, y_pred):\n",
        "    # Compute the predicted token IDs by taking the argmax over the last axis (vocabulary dimension)\n",
        "    y_pred = tf.argmax(y_pred, axis=-1)\n",
        "    # Cast predictions to the same dtype as y_true for comparison\n",
        "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
        "\n",
        "    # Compare predictions to true labels; 1.0 for match, 0.0 otherwise\n",
        "    match = tf.cast(y_true == y_pred, tf.float32)\n",
        "    # Create a mask to ignore padding tokens (where y_true == 0)\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "\n",
        "    # Compute the mean accuracy over all non-padding tokens\n",
        "    return tf.reduce_sum(match) / tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f32GuAhw2nXm"
      },
      "source": [
        "Configure the model for training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9g0DRRvm3l9X"
      },
      "outputs": [],
      "source": [
        "# Compile the model for training.\n",
        "# - optimizer='adam': Use the Adam optimizer for training.\n",
        "# - loss=masked_loss: Use the custom masked loss function to ignore padding tokens.\n",
        "# - metrics=[masked_acc, masked_loss]: Track masked accuracy and masked loss during training.\n",
        "model.compile(optimizer='adam',\n",
        "              loss=masked_loss,\n",
        "              metrics=[masked_acc, masked_loss])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DWLI3pssjnx"
      },
      "source": [
        "The model is randomly initialized, and should give roughly uniform output probabilities. So it's easy to predict what the initial values of the metrics should be:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuP3_LFENMJG"
      },
      "outputs": [],
      "source": [
        "# Get the vocabulary size of the target language as a float\n",
        "vocab_size = 1.0 * target_text_processor.vocabulary_size()\n",
        "\n",
        "# Calculate the expected loss and accuracy for a randomly initialized model:\n",
        "# - expected_loss: log(vocab_size), since the output probabilities are uniform\n",
        "# - expected_acc: 1/vocab_size, since the chance of guessing the correct token is 1 out of vocab_size\n",
        "{\n",
        "    \"expected_loss\": tf.math.log(vocab_size).numpy(),\n",
        "    \"expected_acc\": 1/vocab_size\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frVba49Usd0Z"
      },
      "source": [
        "That should roughly match the values returned by running a few steps of evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rJITfxEsHKR"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the validation dataset.\n",
        "# - val_ds: validation dataset, already processed and batched\n",
        "# - steps=20: run evaluation for 20 batches\n",
        "# - return_dict=True: return the results as a dictionary of metric names and values\n",
        "model.evaluate(val_ds, steps=20, return_dict=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQd_esVVoSf3"
      },
      "outputs": [],
      "source": [
        "# Train the model using the training dataset.\n",
        "# - train_ds.repeat(): Repeat the training dataset indefinitely for multiple epochs.\n",
        "# - epochs=100: Train for up to 100 epochs.\n",
        "# - steps_per_epoch=100: Each epoch consists of 100 batches.\n",
        "# - validation_data=val_ds: Use the validation dataset for evaluation.\n",
        "# - validation_steps=20: Evaluate on 20 batches from the validation set each epoch.\n",
        "# - callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)]: Stop training early if validation loss doesn't improve for 3 epochs.\n",
        "history = model.fit(\n",
        "    train_ds.repeat(),\n",
        "    epochs=100,\n",
        "    steps_per_epoch=100,\n",
        "    validation_data=val_ds,\n",
        "    validation_steps=20,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=3)\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38rLdlmtQHCm"
      },
      "outputs": [],
      "source": [
        "# Plot the training and validation loss curves over epochs.\n",
        "plt.plot(history.history['loss'], label='loss')         # Training loss per epoch\n",
        "plt.plot(history.history['val_loss'], label='val_loss') # Validation loss per epoch\n",
        "\n",
        "# Set the y-axis limits to start at 0 and end at the maximum value currently shown.\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "\n",
        "# Label the x-axis as 'Epoch #' and y-axis as 'CE/token' (cross-entropy per token).\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "\n",
        "# Add a legend to distinguish between training and validation loss curves.\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkhXRASNG80_"
      },
      "outputs": [],
      "source": [
        "# Plot training accuracy over epochs\n",
        "plt.plot(history.history['masked_acc'], label='accuracy')\n",
        "# Plot validation accuracy over epochs\n",
        "plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n",
        "# Set y-axis limits to start at 0 and end at the current maximum\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "# Label the x-axis as 'Epoch #'\n",
        "plt.xlabel('Epoch #')\n",
        "# Label the y-axis as 'CE/token' (cross-entropy per token)\n",
        "plt.ylabel('CE/token')\n",
        "# Add a legend to distinguish between training and validation accuracy\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU3Ce8M6I3rz"
      },
      "source": [
        "### Translate 🌍✨\n",
        "\n",
        "Now that the model is trained, implement a function to execute the full `text => text` translation. This code is basically identical to the [inference example](#inference) in the [decoder section](#the_decoder), but this also captures the attention weights. 🧠🔤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmgYPCVgEwp_"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "@Translator.add_method\n",
        "def translate(self,\n",
        "              texts, *,\n",
        "              max_length=50,\n",
        "              temperature=0.0):\n",
        "  # Process the input texts\n",
        "  context = self.encoder.convert_input(texts)\n",
        "  batch_size = tf.shape(texts)[0]\n",
        "\n",
        "  # Setup the loop inputs\n",
        "  tokens = []\n",
        "  attention_weights = []\n",
        "  next_token, done, state = self.decoder.get_initial_state(context)\n",
        "\n",
        "  for _ in range(max_length):\n",
        "    # Generate the next token\n",
        "    next_token, done, state = self.decoder.get_next_token(\n",
        "        context, next_token, done,  state, temperature)\n",
        "\n",
        "    # Collect the generated tokens\n",
        "    tokens.append(next_token)\n",
        "    attention_weights.append(self.decoder.last_attention_weights)\n",
        "\n",
        "    if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "      break\n",
        "\n",
        "  # Stack the lists of tokens and attention weights.\n",
        "  tokens = tf.concat(tokens, axis=-1)   # t*[(batch 1)] -> (batch, t)\n",
        "  self.last_attention_weights = tf.concat(attention_weights, axis=1)  # t*[(batch 1 s)] -> (batch, t s)\n",
        "\n",
        "  result = self.decoder.tokens_to_text(tokens)\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4XufRntbbva"
      },
      "source": [
        "Here are the two helper methods, used above, to convert tokens to text, and to get the next token:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5hqvbR5FUCD"
      },
      "outputs": [],
      "source": [
        "# Translate the given Spanish sentence using the trained model.\n",
        "result = model.translate(['¿Todavía está en casa?']) # Are you still home\n",
        "\n",
        "# Convert the first result from a TensorFlow tensor to a Python string and print it.\n",
        "result[0].numpy().decode()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ1iU63cVgfs"
      },
      "source": [
        "Use that to generate the attention plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5hQWlbN3jGF"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "@Translator.add_method\n",
        "def plot_attention(self, text, **kwargs):\n",
        "  # Ensure the input is a string\n",
        "  assert isinstance(text, str)\n",
        "  # Translate the input text and get the output sentence\n",
        "  output = self.translate([text], **kwargs)\n",
        "  output = output[0].numpy().decode()\n",
        "\n",
        "  # Get the attention weights for the first example\n",
        "  attention = self.last_attention_weights[0]\n",
        "\n",
        "  # Tokenize and split the input context text\n",
        "  context = tf_lower_and_split_punct(text)\n",
        "  context = context.numpy().decode().split()\n",
        "\n",
        "  # Tokenize and split the output text, skipping the [START] token\n",
        "  output = tf_lower_and_split_punct(output)\n",
        "  output = output.numpy().decode().split()[1:]\n",
        "\n",
        "  # Create a figure for the attention plot\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "  # Display the attention weights as an image\n",
        "  ax.matshow(attention, cmap='viridis', vmin=0.0)\n",
        "\n",
        "  # Set font size for axis labels\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  # Set x-axis labels to context tokens and rotate them\n",
        "  ax.set_xticklabels([''] + context, fontdict=fontdict, rotation=90)\n",
        "  # Set y-axis labels to output tokens\n",
        "  ax.set_yticklabels([''] + output, fontdict=fontdict)\n",
        "\n",
        "  # Set major tick locations to every token\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  # Label the axes\n",
        "  ax.set_xlabel('Input text')\n",
        "  ax.set_ylabel('Output text')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrGawQv2eiA4"
      },
      "outputs": [],
      "source": [
        "# Plot the attention weights for the translation of the given Spanish sentence.\n",
        "# This will visualize which input words the model attends to when generating each output word.\n",
        "model.plot_attention('¿Todavía está en casa?') # Are you still home"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHBdOf9duumm"
      },
      "source": [
        "Translate a few more sentences and plot them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flT0VlQZK11s"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Visualize the attention weights for the translation of the Spanish sentence \"Esta es mi vida.\" \"This is my life.\"\n",
        "# This will show which input words the model attends to when generating each output word.\n",
        "model.plot_attention('Esta es mi vida.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-fPYP_9K8xa"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Visualize the attention weights for the translation of the Spanish sentence \"Tratar de descubrir.\" \"Try to find out.\"\n",
        "# This will show which input words the model attends to when generating each output word.\n",
        "model.plot_attention('Tratar de descubrir.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA3xI3NzrRJt"
      },
      "source": [
        "The short sentences often work well, but if the input is too long the model literally loses focus and stops providing reasonable predictions. There are two main reasons for this:\n",
        "\n",
        "1. The model was trained with teacher-forcing feeding the correct token at each step, regardless of the model's predictions. The model could be made more robust if it were sometimes fed its own predictions. 🏫🤖\n",
        "2. The model only has access to its previous output through the RNN state. If the RNN state looses track of where it was in the context sequence there's no way for the model to recover. [Transformers](transformer.ipynb) improve on this by letting the decoder look at what it has output so far. 🔄🧠✨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vtz6QBoGWqT2"
      },
      "source": [
        "The raw data is sorted by length, so try translating the longest sequence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FUHFLEvSMbG"
      },
      "outputs": [],
      "source": [
        "# Get the longest Spanish sentence from the dataset\n",
        "long_text = context_raw[-1]\n",
        "\n",
        "# Import the textwrap module for formatting long strings\n",
        "import textwrap\n",
        "\n",
        "# Print the expected English translation, wrapped for readability\n",
        "print('Expected output:\\n', '\\n'.join(textwrap.wrap(target_raw[-1])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDa_8NaN_RUy"
      },
      "outputs": [],
      "source": [
        "# Plot the attention weights for the translation of the longest Spanish sentence in the dataset.\n",
        "# This will visualize which input words the model attends to when generating each output word.\n",
        "model.plot_attention(long_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PToqG3GiIUPM"
      },
      "source": [
        "The `translate` function works on batches, so if you have multiple texts to translate you can pass them all at once, which is much more efficient than translating them one at a time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-FLCjBVEMXL"
      },
      "outputs": [],
      "source": [
        "# List of Spanish input sentences to translate\n",
        "inputs = [\n",
        "    'Hace mucho frio aqui.', # \"It's really cold here.\"\n",
        "    'Esta es mi vida.',      # \"This is my life.\"\n",
        "    'Su cuarto es un desastre.' # \"His room is a mess\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sT68i4jYEQ7q"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Loop through each Spanish input sentence in the 'inputs' list\n",
        "for t in inputs:\n",
        "  # Translate the sentence using the trained model and print the result as a decoded string\n",
        "  print(model.translate([t])[0].numpy().decode())\n",
        "\n",
        "print()  # Print a blank line for separation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd2rgyHwVVrv"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Translate a batch of Spanish input sentences using the trained model\n",
        "result = model.translate(inputs)\n",
        "\n",
        "# Print the English translation for each input sentence\n",
        "print(result[0].numpy().decode())\n",
        "print(result[1].numpy().decode())\n",
        "print(result[2].numpy().decode())\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvhMqIw26Bwd"
      },
      "source": [
        "So overall this text generation function mostly gets the job done, but so you've only used it here in python with eager execution. Let's try to export it next:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4POAuUgLxLv"
      },
      "source": [
        "### Save Model 💾✨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-6cFyqeUPQm"
      },
      "source": [
        "If you want to export this model you'll need to wrap the `translate` method in a `tf.function`. That implementation will get the job done:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNhGwQaVKIAy"
      },
      "outputs": [],
      "source": [
        "class Export(tf.Module):\n",
        "  def __init__(self, model):\n",
        "    # Store the trained translation model\n",
        "    self.model = model\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
        "  def translate(self, inputs):\n",
        "    # Exported translation function: takes a batch of strings and returns translations\n",
        "    return self.model.translate(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Tjqs9FzNwW5"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the Export class, wrapping the trained translation model.\n",
        "# This allows you to export the model with a tf.function for inference.\n",
        "export = Export(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkccvHDvXCa8"
      },
      "source": [
        "Run the `tf.function` once to compile it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NzrixLvVBjQ"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Run the exported translation function on a batch of input sentences.\n",
        "# This will compile the tf.function and measure execution time.\n",
        "_ = export.translate(tf.constant(inputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USJdu00tVFbd"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Translate a batch of Spanish input sentences using the exported model.\n",
        "result = export.translate(tf.constant(inputs))\n",
        "\n",
        "# Print the English translation for each input sentence in the batch.\n",
        "print(result[0].numpy().decode())\n",
        "print(result[1].numpy().decode())\n",
        "print(result[2].numpy().decode())\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP2dNtEXJPEL"
      },
      "source": [
        "Now that the function has been traced it can be exported using `saved_model.save`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goMQ0KerIT1C"
      },
      "outputs": [],
      "source": [
        "!export WRAPT_DISABLE_EXTENSIONS=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyvxT5V0_X5B"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "import os\n",
        "# Set the path where the exported translator model will be saved\n",
        "TRANSLATOR_PATH = os.path.join(\"artifacts\", \"translator\")\n",
        "\n",
        "# Export the model using TensorFlow's SavedModel format\n",
        "# - export: the Export wrapper containing the translation model\n",
        "# - TRANSLATOR_PATH: directory to save the model\n",
        "# - signatures: specify the default serving signature for inference\n",
        "tf.saved_model.save(export, TRANSLATOR_PATH,\n",
        "                    signatures={'serving_default': export.translate})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-I0j3i3ekOba"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Load the exported SavedModel from the specified path\n",
        "reloaded = tf.saved_model.load(TRANSLATOR_PATH)\n",
        "\n",
        "# Run the translate function once to warm up the model (compiles the graph for faster inference)\n",
        "_ = reloaded.translate(tf.constant(inputs)) #warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXZF__FZXJCm"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Translate a batch of Spanish input sentences using the reloaded exported model.\n",
        "result = reloaded.translate(tf.constant(inputs))\n",
        "\n",
        "# Print the English translation for each input sentence in the batch.\n",
        "print(result[0].numpy().decode())\n",
        "print(result[1].numpy().decode())\n",
        "print(result[2].numpy().decode())\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pgg3P757O5rw"
      },
      "source": [
        "#### [Optional] Use a dynamic loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3230LfyRIJQV"
      },
      "source": [
        "It's worth noting that this initial implementation is not optimal. It uses a python loop:\n",
        "\n",
        "```\n",
        "for _ in range(max_length):\n",
        "  ...\n",
        "  if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "    break\n",
        "```\n",
        "\n",
        "The python loop is relatively simple but when `tf.function` converts this to a graph, it **statically unrolls** that loop. Unrolling the loop has two disadvantages:\n",
        "\n",
        "1. It makes `max_length` copies of the loop body. So the generated graphs take longer to build, save and load.\n",
        "1. You have to choose a fixed value for the `max_length`.\n",
        "1. You can't `break` from a statically unrolled loop. The `tf.function`\n",
        "  version will run the full `max_length` iterations on every call.\n",
        "  That's why the `break` only works with eager execution. This is\n",
        "  still marginally faster than eager execution, but not as fast as it could be.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPRJp4TRJx_n"
      },
      "source": [
        "To fix these shortcomings, the `translate_dynamic` method, below, uses a tensorflow loop:\n",
        "\n",
        "```\n",
        "for t in tf.range(max_length):\n",
        "  ...\n",
        "  if tf.reduce_all(done):\n",
        "      break\n",
        "```\n",
        "\n",
        "It looks like a python loop, but when you use a tensor as the input to a `for` loop (or the condition of a `while` loop) `tf.function` converts it to a dynamic loop using operations like `tf.while_loop`.\n",
        "\n",
        "There's no need for a `max_length` here it's just in case the model gets stuck generating a loop like: `the united states of the united states of the united states...`.\n",
        "\n",
        "On the down side, to accumulate tokens from this dynamic loop you can't just append them to a python `list`, you need to use a `tf.TensorArray`:\n",
        "\n",
        "```\n",
        "tokens = tf.TensorArray(tf.int64, size=1, dynamic_size=True)\n",
        "...\n",
        "for t in tf.range(max_length):\n",
        "    ...\n",
        "    tokens = tokens.write(t, next_token) # next_token shape is (batch, 1)\n",
        "  ...\n",
        "  tokens = tokens.stack()\n",
        "  tokens = einops.rearrange(tokens, 't batch 1 -> batch t')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTmISp4SRo5U"
      },
      "source": [
        "This version of the code can be quite a bit more efficient:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbQpyYs13jF_"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "@Translator.add_method\n",
        "def translate(self,\n",
        "              texts,\n",
        "              *,\n",
        "              max_length=500,\n",
        "              temperature=tf.constant(0.0)):\n",
        "\n",
        "  # Create a shape checker utility to validate tensor shapes during debugging\n",
        "  shape_checker = ShapeChecker()\n",
        "  # Convert input texts to encoder input format\n",
        "  context = self.encoder.convert_input(texts)\n",
        "  # Get the batch size from the context tensor\n",
        "  batch_size = tf.shape(context)[0]\n",
        "  # Check the shape of the context tensor: (batch, sequence_length, units)\n",
        "  shape_checker(context, 'batch s units')\n",
        "\n",
        "  # Get initial decoder state, start tokens, and done mask for the batch\n",
        "  next_token, done, state = self.decoder.get_initial_state(context)\n",
        "\n",
        "  # Initialize a dynamic TensorArray to accumulate generated tokens\n",
        "  tokens = tf.TensorArray(tf.int64, size=1, dynamic_size=True)\n",
        "\n",
        "  # Loop to generate up to max_length tokens for each sequence in the batch\n",
        "  for t in tf.range(max_length):\n",
        "    # Generate the next token, update done mask and RNN state\n",
        "    next_token, done, state = self.decoder.get_next_token(\n",
        "        context, next_token, done, state, temperature)\n",
        "    # Check the shape of the next token: (batch, 1)\n",
        "    shape_checker(next_token, 'batch t1')\n",
        "\n",
        "    # Write the generated token to the TensorArray\n",
        "    tokens = tokens.write(t, next_token)\n",
        "\n",
        "    # If all sequences are done, break out of the loop early\n",
        "    if tf.reduce_all(done):\n",
        "      break\n",
        "\n",
        "  # Stack the generated tokens into a tensor: (time, batch, 1)\n",
        "  tokens = tokens.stack()\n",
        "  shape_checker(tokens, 't batch t1')\n",
        "  # Rearrange the tensor to shape: (batch, time)\n",
        "  tokens = einops.rearrange(tokens, 't batch 1 -> batch t')\n",
        "  shape_checker(tokens, 'batch t')\n",
        "\n",
        "  # Convert the token IDs back to text strings\n",
        "  text = self.decoder.tokens_to_text(tokens)\n",
        "  shape_checker(text, 'batch')\n",
        "\n",
        "  # Return the generated text for each input sequence\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ_NznOgZTxC"
      },
      "source": [
        "With eager execution this implementation performs on par with the original:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRh66y-YYeBw"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Translate a batch of Spanish input sentences using the trained model\n",
        "result = model.translate(inputs)\n",
        "\n",
        "# Print the English translation for each input sentence in the batch\n",
        "print(result[0].numpy().decode())\n",
        "print(result[1].numpy().decode())\n",
        "print(result[2].numpy().decode())\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6B8W4_MZdX0"
      },
      "source": [
        "But when you wrap it in a `tf.function` you'll notice two differences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQlrhWWrUhgT"
      },
      "outputs": [],
      "source": [
        "class Export(tf.Module):\n",
        "  def __init__(self, model):\n",
        "    # Store the trained translation model\n",
        "    self.model = model\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
        "  def translate(self, inputs):\n",
        "    # Exported translation function: takes a batch of strings and returns translations\n",
        "    return self.model.translate(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pH8yyGHvUmti"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the Export class, wrapping the trained translation model.\n",
        "# This allows you to export the model with a tf.function for inference.\n",
        "export = Export(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnOJvIsvUwBL"
      },
      "source": [
        "First, it's much quicker to trace, since it only creates one copy of the loop body:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CaEbHkwEa1S"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Run the exported translation function on the batch of input sentences.\n",
        "# This will compile and execute the tf.function for inference timing.\n",
        "_ = export.translate(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ABEwtKIZ6eE"
      },
      "source": [
        "The `tf.function` is much faster than running with eager execution, and on small inputs it's often several times faster than the unrolled version, because it can break out of the loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5VdCLxPYrpz"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Translate the batch of Spanish input sentences using the exported model\n",
        "result = export.translate(inputs)\n",
        "\n",
        "# Print the English translation for each input sentence in the batch\n",
        "print(result[0].numpy().decode())\n",
        "print(result[1].numpy().decode())\n",
        "print(result[2].numpy().decode())\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DDmofICJdx0"
      },
      "source": [
        "So save this version as well:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCg7kRq6FVl3"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Set the path for saving the dynamically exported translator model\n",
        "DYNAMIC_TRANSLATOR_PATH = os.path.join(\"artifacts\", \"dynamic_translator\")\n",
        "\n",
        "# Export the model using TensorFlow's SavedModel format\n",
        "# - export: the Export wrapper containing the translation model\n",
        "# - DYNAMIC_TRANSLATOR_PATH: directory to save the model\n",
        "# - signatures: specify the default serving signature for inference\n",
        "tf.saved_model.save(export, DYNAMIC_TRANSLATOR_PATH,\n",
        "                    signatures={'serving_default': export.translate})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrpzxL2vFVl3"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Load the exported dynamic translator model from the specified path\n",
        "reloaded = tf.saved_model.load(DYNAMIC_TRANSLATOR_PATH)\n",
        "\n",
        "# Run the translate function once on the batch of input sentences to warm up the model (compiles the graph for faster inference)\n",
        "_ = reloaded.translate(tf.constant(inputs)) #warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TjSwrCEFVl3"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Translate the batch of Spanish input sentences using the reloaded exported model.\n",
        "result = reloaded.translate(tf.constant(inputs))\n",
        "\n",
        "# Print the English translation for each input sentence in the batch.\n",
        "print(result[0].numpy().decode())\n",
        "print(result[1].numpy().decode())\n",
        "print(result[2].numpy().decode())\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTe5P5ioMJwN"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "* [Download a different dataset](http://www.manythings.org/anki/) to experiment with translations, for example, English to German, or English to French.\n",
        "* Experiment with training on a larger dataset, or using more epochs.\n",
        "* Try the [transformer tutorial](transformer.ipynb) which implements a similar translation task but uses transformer layers instead of RNNs. This version also uses a `text.BertTokenizer` to implement word-piece tokenization.\n",
        "* Visit the [`tensorflow_addons.seq2seq` tutorial](https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt), which demonstrates a higher-level functionality for implementing this sort of sequence-to-sequence model, such as `seq2seq.BeamSearchDecoder`."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv (3.11.13)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

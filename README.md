# ğŸ“Š Project Title: *Transformers And Attention Mechanisms*

This project was originally developed by CompuFlair and modified during their 2025 Bootcamp.

## ğŸ“ Overview

This repository is structured as a multi-layered exploration of sequence modeling and attention-based architectures:

- **ğŸ—“ï¸ Layer 1:** Manual implementation of attention mechanisms and neural machine translation (NMT).
- **ğŸ—“ï¸ Layer 2:** Building Transformer models from scratch using Keras.
- **ğŸ—“ï¸ Layer 3:** Fine-tuning state-of-the-art BERT and GPT models with HuggingFace on the Yelp Review Full dataset.

Each week includes relevant code, experiments, and documentation to guide you through the concepts and practical steps.

---

## Analysis Highlights

### ğŸ”¬ Layer 1: Neural Machine Translation with Attention

Our attention-based NMT model demonstrates sophisticated translation capabilities with detailed performance analysis across different sentence lengths and attention visualization.

#### ğŸ¯ Performance Analysis by Sentence Length
![METEOR Score Analysis by Length](1_nmt_translation/assets/meteor_analysis_by_length.png)
*Figure 1: METEOR score performance breakdown by Spanish sentence length. Shows how translation quality varies from single-word inputs to complex sentences of 10+ words.*

#### ğŸ“ˆ Training Progress
![Training and Validation Loss](1_nmt_translation/assets/training_and_validation_loss.png)
*Figure 2: Training and validation loss curves showing model convergence and learning progress over epochs.*

![Training and Validation Accuracy](1_nmt_translation/assets/training_and_validation_accuracy.png)
*Figure 3: Training and validation accuracy curves demonstrating the model's improving translation performance.*

#### ğŸ” Attention Mechanism Visualization
![Attention Plot - Los Libros](1_nmt_translation/assets/attention_plot_los_libros.png)
*Figure 4: Attention weights visualization for "los libros" â†’ "the books". Shows how the model focuses on relevant Spanish words when generating each English word.*

![Attention Plot - Long Text](1_nmt_translation/assets/attention_plot_long_text.png)
*Figure 5: Attention weights for longer sentence translation, demonstrating the model's ability to handle complex sentence structures and maintain contextual awareness.*

---

## ï¿½ğŸ“ Project Structure


```
Transformers/
â”‚
â”œâ”€â”€ requirements.txt        # List of Python packages needed to run the project
â”œâ”€â”€ README.md               # Project description and usage guide (this file)
â”œâ”€â”€ .gitignore              # Ignore virtual environments, model files, etc. in Git
â”œâ”€â”€ venv/                   # Python virtual environment (should be ignored by git)
â”‚
â”œâ”€â”€ week_1/                 # Week 1: Neural Machine Translation (NMT) with Attention
â”‚   â”œâ”€â”€ nmt_with_attention.ipynb   # Jupyter notebook for building and training an attention-based NMT model
â”‚   â”œâ”€â”€ artifacts/                 # Saved model outputs and experiment results
â”‚   â”‚   â”œâ”€â”€ dynamic_translator/    # Outputs from dynamic translator experiments
â”‚   â”‚   â””â”€â”€ translator/           # Outputs from standard translator experiments
â”‚   â””â”€â”€ README.md                 # Week 1 overview and instructions
â”‚
â”œâ”€â”€ week_2/                 # Week 2: Transformer-based NMT with Keras
â”‚   â”œâ”€â”€ transformer.ipynb         # Jupyter notebook for building and training a Transformer model using Keras
â”‚   â”œâ”€â”€ model.png                 # Architecture diagram of the Transformer model
â”‚   â”œâ”€â”€ artifacts/                # Saved model outputs and experiment results
â”‚   â””â”€â”€ README.md                 # Week 2 overview and instructions
â”‚
â”œâ”€â”€ week_3/                 # Week 3: Fine-tuning BERT and GPT on Yelp Reviews
â”‚   â””â”€â”€ fine_tune_hug.ipynb       # Notebook for fine-tuning and evaluating BERT/GPT models on Yelp Review Full dataset
â”‚   â”‚                             # Includes data loading, tokenization, training, evaluation, and comparison of architectures
â”‚   â””â”€â”€ README.md                 # Week 3 overview and instructions
```

---

## ğŸ”§ Setup and Installation Instructions

### ğŸš€ OPTION 1: Use Google Colab

For a quick start, open the notebooks directly in [Google Colab](https://colab.research.google.com/) or upload the notebook files from this repository to your Colab workspace.

### ğŸ’» OPTION 2: Run Locally

#### 1ï¸âƒ£ Create a virtual environment and activate it

It's recommended to create a different environment for each week to avoid conflicts.

#### ğŸ–¥ï¸ macOS & Linux

```bash
python3 -m venv .venv
source .venv/bin/activate
```

#### ğŸªŸ Windows

```bash
python -m venv .venv
.venv\Scripts\activate
```

#### ğŸ“¦ 2. Install dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ› ï¸ Step-by-Step Guide

Check every week content.

### Step [1]:
Go to [Week 1](week_1/README.md) folder and check the readme to proceed forward.


### Step [2]:
Go to [Week 2](week_2/README.md) folder and check the readme to proceed forward.

### Step [3]:
Go to [Week 3](week_3/README.md) folder and check the readme to proceed forward.

---

## ğŸ† Results

We successfully implemented the Transformer architecture and explored attention mechanisms in depth. By the final week, we fine-tuned cutting-edge GPT ğŸ¤– and BERT ğŸ§  models using HuggingFace on the Yelp Review Full dataset. The journey was both challenging and rewarding, offering hands-on experience with state-of-the-art NLP techniques and delivering impressive results on real-world data ğŸ“ˆ.

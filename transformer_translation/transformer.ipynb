{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a934948f7030"
      },
      "source": [
        "# Neural machine translation with a Transformer and Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a241496dc3d9"
      },
      "source": [
        "## Overview üöÄ\n",
        "\n",
        "This notebook offers a hands-on tutorial for building a neural machine translation model using the Transformer architecture with Keras and TensorFlow.\n",
        "\n",
        "You'll be guided through:\n",
        "- üì¶ Preparing and tokenizing a Portuguese-English translation dataset.\n",
        "- üèóÔ∏è Constructing essential model components, including positional embeddings and attention layers.\n",
        "- üß© Assembling a multi-layer Transformer encoder-decoder model.\n",
        "\n",
        "By the end, you will:\n",
        "- üèãÔ∏è‚Äç‚ôÇÔ∏è Train the model.\n",
        "- üåç Generate translations.\n",
        "- üîç Visualize attention mechanisms.\n",
        "\n",
        "Complex concepts are broken down into clear, manageable steps, making this notebook accessible for anyone interested in state-of-the-art sequence-to-sequence models for natural language processing tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCg3ElKBUSBb"
      },
      "source": [
        "## Goal üéØ\n",
        "\n",
        "This tutorial demonstrates how to create and train a [sequence-to-sequence](https://developers.google.com/machine-learning/glossary#sequence-to-sequence-task) [Transformer](https://developers.google.com/machine-learning/glossary#Transformer) model to translate [Portuguese into English](https://www.tensorflow.org/datasets/catalog/ted_hrlr_translate#ted_hrlr_translatept_to_en). The Transformer was originally proposed in [\"Attention is all you need\"](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017). üåçüîÑ\n",
        "\n",
        "Transformers are deep neural networks that replace CNNs and RNNs with [self-attention](https://developers.google.com/machine-learning/glossary#self-attention). ü§ñ‚ú® Self-attention allows Transformers to easily transmit information across the input sequences.\n",
        "\n",
        "As explained in the [Google AI Blog post](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html):\n",
        "\n",
        "> Neural networks for machine translation typically contain an encoder reading the input sentence and generating a representation of it. A decoder then generates the output sentence word by word while consulting the representation generated by the encoder. The Transformer starts by generating initial representations, or embeddings, for each word... Then, using self-attention, it aggregates information from all of the other words, generating a new representation per word informed by the entire context, represented by the filled balls. This step is then repeated multiple times in parallel for all words, successively generating new representations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo1P7AN4lzdi"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/apply_the_transformer_to_machine_translation.gif\" alt=\"Applying the Transformer to machine translation\">\n",
        "\n",
        "Figure 1: Applying the Transformer to machine translation. Source: [Google AI Blog](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAxfGTJJYbQi"
      },
      "source": [
        "That's a lot to digest! The goal of this tutorial is to break it down into easy to understand parts. In this tutorial you will:\n",
        "\n",
        "- üì¶ Prepare the data.\n",
        "- üèóÔ∏è Implement necessary components:\n",
        "  - üß© Positional embeddings.\n",
        "  - üéØ Attention layers.\n",
        "  - üèõÔ∏è The encoder and decoder.\n",
        "- ü§ñ Build & train the Transformer.\n",
        "- üåç Generate translations.\n",
        "- üì¶ Export the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddvOmAXhaDXt"
      },
      "source": [
        "A Transformer is a sequence-to-sequence encoder-decoder model similar to the model in week 1. ü§ñüîÑ\n",
        "\n",
        "A single-layer Transformer takes a little more code to write, but is almost identical to that encoder-decoder RNN model. The only difference is that the RNN layers are replaced with self-attention layers. ‚ú®\n",
        "This tutorial builds a 4-layer Transformer which is larger and more powerful, but not fundamentally more complex. üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk40oPm8OD51"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th>The <a href=https://www.tensorflow.org/text/tutorials/nmt_with_attention>RNN+Attention model</a></th>\n",
        "  <th>A 1-layer transformer</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=411 src=\"https://www.tensorflow.org/images/tutorials/transformer/RNN+attention-words.png\"/>\n",
        "  </td>\n",
        "  <td>\n",
        "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-1layer-words.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huJ97Eh-Ue4V"
      },
      "source": [
        "After training the model in this notebook, you will be able to input a Portuguese sentence and return the English translation.\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/attention_map_portuguese.png\" alt=\"Attention heatmap\">\n",
        "\n",
        "Figure 2: Visualized attention weights that you can generate at the end of this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZL6uTTE5137"
      },
      "source": [
        "## Why Transformers are significant ü§ñ‚ú®\n",
        "\n",
        "- Transformers excel at modeling sequential data, such as natural language. üó£Ô∏è\n",
        "- Unlike [recurrent neural networks (RNNs)](./text_generation.ipynb), Transformers are parallelizable. This makes them efficient on hardware like GPUs and TPUs. ‚ö° The main reason is that Transformers replaced recurrence with attention, and computations can happen simultaneously. Layer outputs can be computed in parallel, instead of a series like an RNN.\n",
        "- Unlike [RNNs](https://www.tensorflow.org/guide/keras/rnn) (such as [seq2seq, 2014](https://arxiv.org/abs/1409.3215)) or [convolutional neural networks (CNNs)](https://www.tensorflow.org/tutorials/images/cnn) (for example, [ByteNet](https://arxiv.org/abs/1610.10099)), Transformers are able to capture distant or long-range contexts and dependencies in the data between distant positions in the input or output sequences. Thus, longer connections can be learned. üîó Attention allows each location to have access to the entire input at each layer, while in RNNs and CNNs, the information needs to pass through many processing steps to move a long distance, which makes it harder to learn.\n",
        "- Transformers make no assumptions about the temporal/spatial relationships across the data. This is ideal for processing a set of objects (for example, [StarCraft units](https://www.deepmind.com/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii)). üïπÔ∏è\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/encoder_self_attention_distribution.png\" width=\"800\" alt=\"Encoder self-attention distribution for the word it from the 5th to the 6th layer of a Transformer trained on English-to-French translation\">\n",
        "\n",
        "Figure 3: The encoder self-attention distribution for the word ‚Äúit‚Äù from the 5th to the 6th layer of a Transformer trained on English-to-French translation (one of eight attention heads). Source: [Google AI Blog](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swymtxpl7W7w"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfV1batAwq9j"
      },
      "source": [
        "Begin by installing [TensorFlow Datasets](https://tensorflow.org/datasets) for loading the dataset and [TensorFlow Text](https://www.tensorflow.org/text) for text preprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFG0NDRu5mYQ"
      },
      "outputs": [],
      "source": [
        "# # Install a specific version of CUDA's cuDNN library required for TensorFlow GPU support.\n",
        "# !apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
        "\n",
        "# Uninstall any existing versions of TensorFlow, Keras, TensorFlow Estimator, and TensorFlow Text\n",
        "# to avoid conflicts and ensure a clean environment.\n",
        "!pip uninstall -y -q tensorflow keras tensorflow-estimator tensorflow-text\n",
        "\n",
        "# Install a compatible version of protobuf, which is required by TensorFlow and TensorFlow Datasets.\n",
        "!pip install protobuf~=3.20.3\n",
        "\n",
        "# Install TensorFlow Datasets for easy access to pre-built datasets.\n",
        "!pip install -q tensorflow_datasets\n",
        "\n",
        "# Install or upgrade TensorFlow and TensorFlow Text to the latest versions.\n",
        "# TensorFlow Text provides text processing ops compatible with TensorFlow.\n",
        "!pip install -q -U tensorflow-text tensorflow\n",
        "\n",
        "!pip install matplotlib\n",
        "!pip install wrapt==1.15.0\n",
        "!export WRAPT_DISABLE_EXTENSIONS=true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GYpLBSjxJmG"
      },
      "source": [
        "Import the necessary modules:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjJJyJTZYebt"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_text\n",
        "import wrapt\n",
        "if wrapt.__version__!=\"1.15.0\" or tf.__version__!=\"2.19.1\":\n",
        "  raise Exception(f\"Please restart your session as you are still using warpt version: {wrapt.__version__} and tensorflow: {tf.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf_WUi2HLhzf"
      },
      "source": [
        "## Data handling üì¶\n",
        "\n",
        "This section downloads the dataset and the subword tokenizer, from [this tutorial](https://www.tensorflow.org/text/guide/subwords_tokenizer), then wraps it all up in a `tf.data.Dataset` for training.\n",
        "\n",
        " <section class=\"expandable tfo-display-only-on-site\">\n",
        " <button type=\"button\" class=\"button-red button expand-control\">Toggle section</button>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cCvXbPkccV1"
      },
      "source": [
        "### Download the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTEVgBxklzdq"
      },
      "source": [
        "Use TensorFlow Datasets to load the [Portuguese-English translation dataset](https://www.tensorflow.org/datasets/catalog/ted_hrlr_translate#ted_hrlr_translatept_to_en)D Talks Open Translation Project. This dataset contains approximately 52,000 training, 1,200 validation and 1,800 test examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8q9t4FmN96eN"
      },
      "outputs": [],
      "source": [
        "# Load the TED Talks Portuguese-English translation dataset using TensorFlow Datasets.\n",
        "# 'with_info=True' returns the dataset and its metadata (such as feature info, splits, etc.).\n",
        "# 'as_supervised=True' returns each example as a tuple (input, target) instead of a dictionary.\n",
        "\n",
        "examples, metadata = tfds.load(\n",
        "    'ted_hrlr_translate/pt_to_en',  # Dataset name: TED Talks Portuguese to English\n",
        "    with_info=True,                 # Also return metadata about the dataset\n",
        "    as_supervised=True              # Return examples as (input, target) pairs\n",
        ")\n",
        "\n",
        "# Split the loaded dataset into training and validation sets.\n",
        "train_examples = examples['train']        # Training set: used to train the model\n",
        "val_examples = examples['validation']     # Validation set: used to evaluate model performance during training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA4cw7F_DmSt"
      },
      "source": [
        "The `tf.data.Dataset` object returned by TensorFlow Datasets yields pairs of text examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZFAMZJyDrFn"
      },
      "outputs": [],
      "source": [
        "# Iterate over a single batch of 3 examples from the training dataset.\n",
        "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
        "  # Print the Portuguese examples in the batch.\n",
        "  print('> Examples in Portuguese:')\n",
        "  for pt in pt_examples.numpy():\n",
        "    # Decode the byte string to a regular string for display.\n",
        "    print(pt.decode('utf-8'))\n",
        "  print()\n",
        "\n",
        "  # Print the corresponding English examples in the batch.\n",
        "  print('> Examples in English:')\n",
        "  for en in en_examples.numpy():\n",
        "    # Decode the byte string to a regular string for display.\n",
        "    print(en.decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJxTd6aVnZyh"
      },
      "source": [
        "### Set up the tokenizer üß©"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mopr6oKUlzds"
      },
      "source": [
        "Now that you have loaded the dataset, you need to tokenize the text, so that each element is represented as a [token](https://developers.google.com/machine-learning/glossary#token) or token ID (a numeric representation). ‚úÇÔ∏èüî¢\n",
        "\n",
        "Tokenization is the process of breaking up text, into \"tokens\". Depending on the tokenizer, these tokens can represent sentence-pieces, words, subwords, or characters. üß© To learn more about tokenization, visit [this guide](https://www.tensorflow.org/text/guide/tokenizers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJr_8Jz9FKgu"
      },
      "source": [
        "This tutorial uses the tokenizers built in the [subword tokenizer](https://www.tensorflow.org/text/guide/subwords_tokenizer) tutorial. That tutorial optimizes two `text.BertTokenizer` objects (one for English, one for Portuguese) for **this dataset** and exports them in a TensorFlow `saved_model` format. üß©üî§\n",
        "\n",
        "> Note: This is different from the [original paper](https://arxiv.org/pdf/1706.03762.pdf), section 5.1, where they used a single byte-pair tokenizer for both the source and target with a vocabulary-size of 37000.\n",
        "\n",
        "Download, extract, and import the `saved_model`: üì¶‚¨áÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QToMl0NanZPr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define the directory where artifacts (such as downloaded models) will be stored.\n",
        "SAVE_DIR = 'artifacts/.'\n",
        "\n",
        "# If the directory does not exist, create it.\n",
        "if not os.path.exists(SAVE_DIR):\n",
        "    os.makedirs(SAVE_DIR)\n",
        "\n",
        "# Name of the tokenizer model to download.\n",
        "model_name = 'ted_hrlr_translate_pt_en_converter'\n",
        "\n",
        "# Download the tokenizer model zip file from TensorFlow's public storage.\n",
        "# tf.keras.utils.get_file will:\n",
        "#   - Download the file if it does not exist in the cache.\n",
        "#   - Store it in the specified cache_dir and cache_subdir.\n",
        "#   - Extract the contents of the zip file after downloading.\n",
        "tf.keras.utils.get_file(\n",
        "    f'{model_name}.zip',  # Name for the downloaded file.\n",
        "    f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',  # URL to download from.\n",
        "    cache_dir=SAVE_DIR,   # Directory to cache the file.\n",
        "    cache_subdir='.',     # Subdirectory within the cache directory.\n",
        "    extract=True          # Automatically extract the zip file after download.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5dbGnPXnuI1"
      },
      "outputs": [],
      "source": [
        "# Construct the path to the tokenizer SavedModel directory.\n",
        "# The tokenizer was downloaded and extracted in the previous cell.\n",
        "# SAVE_DIR: Directory where artifacts are stored (created earlier).\n",
        "# model_name: Name of the tokenizer model (set earlier).\n",
        "# The \"_extracted\" subdirectory is created by tf.keras.utils.get_file(extract=True).\n",
        "TOKENIZER_PATH = os.path.join(SAVE_DIR, f\"{model_name}_extracted\", model_name)\n",
        "\n",
        "# Load the tokenizer SavedModel from the constructed path.\n",
        "# This loads a TensorFlow SavedModel containing two tokenizers:\n",
        "#   - tokenizers.pt: Portuguese tokenizer\n",
        "#   - tokenizers.en: English tokenizer\n",
        "# Each tokenizer provides methods for tokenization, detokenization, and vocabulary lookup.\n",
        "tokenizers = tf.saved_model.load(TOKENIZER_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CexgkIS1lzdt"
      },
      "source": [
        "The `tf.saved_model` contains two text tokenizers, one for English and one for Portuguese. Both have the same methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-PCJijfcZ9_"
      },
      "outputs": [],
      "source": [
        "# List all public attributes and methods of the English tokenizer object.\n",
        "# The dir() function returns all attributes (including methods) of an object.\n",
        "# The list comprehension filters out any attribute names that start with an underscore ('_'),\n",
        "# which are typically private or internal attributes in Python.\n",
        "[item for item in dir(tokenizers.en) if not item.startswith('_')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUBljDDEFWUC"
      },
      "source": [
        "The `tokenize` method converts a batch of strings to a padded-batch of token IDs. This method splits punctuation, lowercases and unicode-normalizes the input before tokenizing. That standardization is not visible here because the input data is already standardized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_gPC5iwFXfU"
      },
      "outputs": [],
      "source": [
        "# Print a batch of English strings from the dataset.\n",
        "print('> This is a batch of strings:')\n",
        "for en in en_examples.numpy():\n",
        "  # Each element in en_examples is a byte string, so decode it to a regular string for display.\n",
        "  print(en.decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSkM7z8JFaVO"
      },
      "outputs": [],
      "source": [
        "# Tokenize a batch of English examples using the loaded tokenizer.\n",
        "# This converts each string in en_examples to a sequence of token IDs.\n",
        "encoded = tokenizers.en.tokenize(en_examples)\n",
        "\n",
        "print('> This is a padded-batch of token IDs:')\n",
        "# Iterate through each row (example) in the tokenized batch.\n",
        "for row in encoded.to_list():\n",
        "  # Print the list of token IDs for each example.\n",
        "  print(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBkv7XeBFa8_"
      },
      "source": [
        "The `detokenize` method attempts to convert these token IDs back to human-readable text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CFS5aAxFdpP"
      },
      "outputs": [],
      "source": [
        "# Detokenize the batch of token IDs back into human-readable text.\n",
        "# 'encoded' is a batch of token IDs produced by the tokenizer.\n",
        "# 'tokenizers.en.detokenize' converts these token IDs back to strings.\n",
        "round_trip = tokenizers.en.detokenize(encoded)\n",
        "\n",
        "print('> This is human-readable text:')\n",
        "# Iterate through each detokenized string in the batch.\n",
        "for line in round_trip.numpy():\n",
        "  # Each line is a byte string, so decode it to a regular string for display.\n",
        "  print(line.decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-2gMSBBU-AE"
      },
      "source": [
        "The lower level `lookup` method converts from token-IDs to token text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaCeOnswVAhI"
      },
      "outputs": [],
      "source": [
        "# Print a header to indicate what is being displayed.\n",
        "print('> This is the text split into tokens:')\n",
        "\n",
        "# Use the English tokenizer's 'lookup' method to convert token IDs (from 'encoded')\n",
        "# back into their corresponding token strings. This returns a RaggedTensor of tokens.\n",
        "tokens = tokenizers.en.lookup(encoded)\n",
        "\n",
        "# Display the resulting tokens. Each example in the batch is shown as a list of tokens.\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR3vZJf1Yhg_"
      },
      "source": [
        "The output demonstrates the \"subword\" aspect of the subword tokenization.\n",
        "\n",
        "For example, the word `'searchability'` is decomposed into `'search'` and `'##ability'`, and the word `'serendipity'` into `'s'`, `'##ere'`, `'##nd'`, `'##ip'` and `'##ity'`.\n",
        "\n",
        "Note that the tokenized text includes `'[START]'` and `'[END]'` tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_4vdnhSaATh"
      },
      "source": [
        "The distribution of tokens per example in the dataset is as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRbke-iaaHFI"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty list to store the lengths of tokenized sequences.\n",
        "lengths = []\n",
        "\n",
        "# Iterate over the training examples in batches of 1024.\n",
        "for pt_examples, en_examples in train_examples.batch(1024):\n",
        "  # Tokenize the batch of Portuguese examples using the pretrained tokenizer.\n",
        "  pt_tokens = tokenizers.pt.tokenize(pt_examples)\n",
        "  # Append the lengths of each tokenized Portuguese sequence in the batch to the 'lengths' list.\n",
        "  lengths.append(pt_tokens.row_lengths())\n",
        "\n",
        "  # Tokenize the batch of English examples using the pretrained tokenizer.\n",
        "  en_tokens = tokenizers.en.tokenize(en_examples)\n",
        "  # Append the lengths of each tokenized English sequence in the batch to the 'lengths' list.\n",
        "  lengths.append(en_tokens.row_lengths())\n",
        "\n",
        "  # Print a dot for each batch processed, to indicate progress.\n",
        "  print('.', end='', flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ucA1q3GaK_n"
      },
      "outputs": [],
      "source": [
        "# Concatenate all tokenized sequence lengths from the 'lengths' list into a single numpy array.\n",
        "all_lengths = np.concatenate(lengths)\n",
        "\n",
        "# Plot a histogram of the token counts per example.\n",
        "# The bins are set from 0 to 500, with 101 intervals.\n",
        "plt.hist(all_lengths, np.linspace(0, 500, 101))\n",
        "\n",
        "# Set the y-axis limits to the current limits (this line is redundant but preserves the current view).\n",
        "plt.ylim(plt.ylim())\n",
        "\n",
        "# Find the maximum token count in the dataset.\n",
        "max_length = max(all_lengths)\n",
        "\n",
        "# Draw a vertical line at the maximum token count to highlight it on the histogram.\n",
        "plt.plot([max_length, max_length], plt.ylim())\n",
        "\n",
        "# Add a title to the plot showing the maximum number of tokens per example.\n",
        "plt.title(f'Maximum tokens per example: {max_length}');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Yb35sTJcZq9"
      },
      "source": [
        "### Set up a data pipeline with `tf.data` üìäüîÑ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZHsns5obJhN"
      },
      "source": [
        "The following function takes batches of text as input, and converts them to a format suitable for training. üìù‚û°Ô∏èü§ñ\n",
        "\n",
        "1. It tokenizes them into ragged batches. ‚úÇÔ∏è\n",
        "2. It trims each to be no longer than `MAX_TOKENS`. ‚úÇÔ∏èüî¢\n",
        "3. It splits the target (English) tokens into inputs and labels. These are shifted by one step so that at each input location the `label` is the id of the next token. üîÑ\n",
        "4. It converts the `RaggedTensor`s to padded dense `Tensor`s. üß©\n",
        "5. It returns an `(inputs, labels)` pair. üéØ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6shgzEck3FiV"
      },
      "outputs": [],
      "source": [
        "# Set the maximum number of tokens for input and output sequences.\n",
        "MAX_TOKENS = 128\n",
        "\n",
        "def prepare_batch(pt, en):\n",
        "    # Tokenize the batch of Portuguese sentences.\n",
        "    # Output is a RaggedTensor of token IDs.\n",
        "    pt = tokenizers.pt.tokenize(pt)\n",
        "    # Trim each Portuguese sequence to a maximum of MAX_TOKENS tokens.\n",
        "    pt = pt[:, :MAX_TOKENS]\n",
        "    # Convert the RaggedTensor to a dense Tensor, padding with zeros as needed.\n",
        "    pt = pt.to_tensor()\n",
        "\n",
        "    # Tokenize the batch of English sentences.\n",
        "    en = tokenizers.en.tokenize(en)\n",
        "    # Trim each English sequence to a maximum of MAX_TOKENS+1 tokens.\n",
        "    # The extra token is for shifting during input/label creation.\n",
        "    en = en[:, :(MAX_TOKENS + 1)]\n",
        "    # Prepare the decoder input by removing the last token ([END]).\n",
        "    en_inputs = en[:, :-1].to_tensor()\n",
        "    # Prepare the decoder labels by removing the first token ([START]).\n",
        "    en_labels = en[:, 1:].to_tensor()\n",
        "\n",
        "    # Return a tuple: ((Portuguese tokens, English input tokens), English label tokens)\n",
        "    # This matches the expected input format for training a sequence-to-sequence model.\n",
        "    return (pt, en_inputs), en_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAroQ6xelzdx"
      },
      "source": [
        "The function below converts a dataset of text examples into data of batches for training. üìù‚û°Ô∏èüì¶\n",
        "\n",
        "1. It tokenizes the text, and filters out the sequences that are too long. ‚úÇÔ∏è\n",
        "   (The `batch`/`unbatch` is included because the tokenizer is much more efficient on large batches).\n",
        "2. The `cache` method ensures that that work is only executed once. üóÉÔ∏è\n",
        "3. Then `shuffle` and, `dense_to_ragged_batch` randomize the order and assemble batches of examples. üîÄ\n",
        "4. Finally `prefetch` runs the dataset in parallel with the model to ensure that data is available when needed. ‚ö° See [Better performance with the `tf.data`](https://www.tensorflow.org/guide/data_performance.ipynb) for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcRp7VcQ5m6g"
      },
      "outputs": [],
      "source": [
        "# Set the buffer size for shuffling the dataset.\n",
        "# A larger buffer size means better shuffling, but uses more memory.\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "# Set the batch size for training.\n",
        "# This determines how many examples are processed together in one training step.\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUN_jLBTwNxk"
      },
      "outputs": [],
      "source": [
        "def make_batches(ds):\n",
        "  # Shuffle the dataset with a buffer size for randomness.\n",
        "  # This helps ensure that batches are not correlated and improves training.\n",
        "  return (\n",
        "      ds\n",
        "      .shuffle(BUFFER_SIZE)  # Randomly shuffle the dataset using the specified buffer size.\n",
        "      .batch(BATCH_SIZE)     # Group the dataset into batches of size BATCH_SIZE.\n",
        "      # Map the prepare_batch function to each batch.\n",
        "      # prepare_batch tokenizes, trims, and formats the data for training.\n",
        "      .map(prepare_batch, tf.data.AUTOTUNE)\n",
        "      # Prefetch allows data loading and processing to happen asynchronously,\n",
        "      # so the model always has data ready for training, improving performance.\n",
        "      .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itSWqk-ivrRg"
      },
      "source": [
        "## Test the Dataset üß™‚ú®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSswr5TKvoNM"
      },
      "outputs": [],
      "source": [
        "# Create training and validation set batches for the Transformer model.\n",
        "#\n",
        "# The make_batches function:\n",
        "#   - Shuffles the dataset for randomness (improves training).\n",
        "#   - Batches the data into groups of BATCH_SIZE examples.\n",
        "#   - Tokenizes and formats each batch using prepare_batch (converts text to token IDs, trims, splits, pads).\n",
        "#   - Prefetches batches for efficient input pipeline (overlaps data preparation and model execution).\n",
        "#\n",
        "# train_batches: tf.data.Dataset of (inputs, labels) pairs for training.\n",
        "# val_batches: tf.data.Dataset of (inputs, labels) pairs for validation.\n",
        "# Each 'inputs' is a tuple of (Portuguese tokens, English input tokens).\n",
        "# Each 'labels' is the English target tokens, shifted by one position for teacher forcing.\n",
        "train_batches = make_batches(train_examples)\n",
        "val_batches = make_batches(val_examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSufllC7wooA"
      },
      "source": [
        "The resulting `tf.data.Dataset` objects are setup for training with Keras.  \n",
        "Keras `Model.fit` training expects `(inputs, labels)` pairs.  \n",
        "The `inputs` are pairs of tokenized Portuguese and English sequences, `(pt, en)`.  \n",
        "The `labels` are the same English sequences shifted by 1.  \n",
        "This shift is so that at each location input `en` sequence, the `label` in the next token.  \n",
        "\n",
        "üßë‚Äçüíª‚û°Ô∏èü§ñ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJdJttsF751"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th>Inputs at the bottom, labels at the top.</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-1layer-words.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsF751JJdJt"
      },
      "source": [
        "This is the same as the [text generation tutorial](text_generation.ipynb) ‚úçÔ∏è,\n",
        "except here you have additional input \"context\" (the Portuguese sequence) that the model is \"conditioned\" on. üåê‚û°Ô∏èüá¨üáß\n",
        "\n",
        "This setup is called \"teacher forcing\" üë®‚Äçüè´ because regardless of the model's output at each timestep, it gets the true value as input for the next timestep.\n",
        "This is a simple and efficient way to train a text generation model. ‚ö°\n",
        "It's efficient because you don't need to run the model sequentially, the outputs at the different sequence locations can be computed in parallel. üèÉ‚Äç‚ôÇÔ∏èüèÉ‚Äç‚ôÄÔ∏è\n",
        "\n",
        "You might have expected the `input, output`, pairs to simply be the `Portuguese, English` sequences. üáµüáπ‚û°Ô∏èüá¨üáß\n",
        "Given the Portuguese sequence, the model would try to generate the English sequence.\n",
        "\n",
        "It's possible to train a model that way. You'd need to write out the inference loop and pass the model's output back to the input. üîÑ\n",
        "It's slower (time steps can't run in parallel), and a harder task to learn (the model can't get the end of a sentence right until it gets the beginning right), üê¢\n",
        "but it can give a more stable model because the model has to learn to correct its own errors during training. üõ†Ô∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAw2XjRwLFWr"
      },
      "outputs": [],
      "source": [
        "# Iterate over one batch from the training dataset.\n",
        "# train_batches yields ((pt, en), en_labels) for each batch:\n",
        "#   - pt: Portuguese token IDs, shape (batch_size, seq_len)\n",
        "#   - en: English input token IDs, shape (batch_size, seq_len)\n",
        "#   - en_labels: English target token IDs, shape (batch_size, seq_len)\n",
        "for (pt, en), en_labels in train_batches.take(1):\n",
        "  # 'break' is used to exit after the first batch is retrieved.\n",
        "  break\n",
        "\n",
        "# Print the shapes of the token tensors in the batch.\n",
        "# These shapes help verify the batch dimensions and sequence lengths.\n",
        "print(pt.shape)        # Shape of Portuguese token batch (batch_size, seq_len)\n",
        "print(en.shape)        # Shape of English input token batch (batch_size, seq_len)\n",
        "print(en_labels.shape) # Shape of English label token batch (batch_size, seq_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tzo3JKaqx46g"
      },
      "source": [
        "The `en` and `en_labels` are the same, just shifted by 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apFeC-WWxzR4"
      },
      "outputs": [],
      "source": [
        "# Print the first 10 token IDs from the first example in the English input batch.\n",
        "# 'en' contains the input token IDs for the decoder (English), shape: (batch_size, seq_len).\n",
        "print(en[0][:10])\n",
        "\n",
        "# Print the first 10 token IDs from the first example in the English label batch.\n",
        "# 'en_labels' contains the target token IDs for the decoder (English), shape: (batch_size, seq_len).\n",
        "# These are shifted by one position compared to 'en', so each label is the next token for each input position.\n",
        "print(en_labels[0][:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e7hKcxn6-zd"
      },
      "source": [
        "## Define the components üß©"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVE5j6JlcAps"
      },
      "source": [
        "There's a lot going on inside a Transformer. ü§ñ The important things to remember are:\n",
        "\n",
        "1. It follows the same general pattern as a standard sequence-to-sequence model with an encoder and a decoder. üîÑ\n",
        "2. If you work through it step by step it will all make sense. ü™ú‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0R4bYJ0DiFR"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The original Transformer diagram</th>\n",
        "  <th colspan=1>A representation of a 4-layer Transformer</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\"/>\n",
        "  </td>\n",
        "  <td>\n",
        "   <img width=307 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-4layer-compact.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "Each of the components in these two diagrams will be explained as you progress through the tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS75Y-9-lkzn"
      },
      "source": [
        "### The embedding and positional encoding layer üß©‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26l90xiq3Nis"
      },
      "source": [
        "The inputs to both the encoder and decoder use the same embedding and positional encoding logic. ‚ú®üî¢\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The embedding and positional encoding layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/PositionalEmbedding.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "279u2DiDlmdS"
      },
      "source": [
        "Given a sequence of tokens, both the input tokens (Portuguese) and target tokens (English) have to be converted to vectors using a `tf.keras.layers.Embedding` layer. üß©üî¢\n",
        "\n",
        "The attention layers used throughout the model see their input as a set of vectors, with no order. Since the model doesn't contain any recurrent or convolutional layers, it needs some way to identify word order, otherwise it would see the input sequence as a [bag of words](https://developers.google.com/machine-learning/glossary#bag-of-words) instance ‚Äî `how are you`, `how you are`, `you how are`, and so on, are indistinguishable. üëú\n",
        "\n",
        "A Transformer adds a \"Positional Encoding\" to the embedding vectors. It uses a set of sines and cosines at different frequencies (across the sequence). By definition, nearby elements will have similar position encodings. üåäüî¢"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gcCNZP7lzdy"
      },
      "source": [
        "The original paper uses the following formula for calculating the positional encoding: ‚ú®\n",
        "\n",
        "$$\\Large{PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})} $$\n",
        "$$\\Large{PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})} $$\n",
        "\n",
        "> üìù Note: The code below implements it, but instead of interleaving the sines and cosines, the vectors of sines and cosines are simply concatenated. Permuting the channels like this is functionally equivalent, and just a little easier to implement and show in the plots below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Rz82wEs5biZ"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(length, depth):\n",
        "  # The positional encoding uses sine and cosine functions at different frequencies.\n",
        "  # The depth is split in half for sine and cosine components.\n",
        "  depth = depth / 2\n",
        "\n",
        "  # Create a column vector of positions (sequence indices), shape: (length, 1)\n",
        "  positions = np.arange(length)[:, np.newaxis]\n",
        "\n",
        "  # Create a row vector of normalized depths, shape: (1, depth)\n",
        "  # This determines the frequency for each dimension.\n",
        "  depths = np.arange(depth)[np.newaxis, :] / depth\n",
        "\n",
        "  # Calculate the angle rates for each depth dimension.\n",
        "  # This controls the frequency of the sine/cosine for each channel.\n",
        "  angle_rates = 1 / (10000 ** depths)  # shape: (1, depth)\n",
        "\n",
        "  # Compute the angle radians for each position and depth.\n",
        "  # This is the core of the positional encoding formula.\n",
        "  angle_rads = positions * angle_rates  # shape: (length, depth)\n",
        "\n",
        "  # Concatenate the sine and cosine encodings along the last axis.\n",
        "  # This doubles the depth, matching the original embedding dimension.\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1\n",
        "  )\n",
        "\n",
        "  # Convert the numpy array to a TensorFlow tensor of type float32.\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra1IcbzFhnmF"
      },
      "source": [
        "The position encoding function is a stack of sines and cosines that vibrate at different frequencies depending on their location along the depth of the embedding vector. They vibrate across the position axis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKf4Ky2dhg0L"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Generate positional encoding for a sequence of length 2048 and embedding depth 512.\n",
        "# This encoding will be used to inject positional information into token embeddings.\n",
        "pos_encoding = positional_encoding(length=2048, depth=512)\n",
        "\n",
        "# Print the shape of the positional encoding tensor.\n",
        "# Expected shape: (2048, 512), where 2048 is the sequence length and 512 is the embedding dimension.\n",
        "print(pos_encoding.shape)\n",
        "\n",
        "# Visualize the positional encoding matrix.\n",
        "# Transpose the matrix so that each row corresponds to a depth dimension and each column to a position.\n",
        "# Use a color mesh plot to show how the encoding values vary across positions and embedding dimensions.\n",
        "plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\n",
        "plt.ylabel('Depth')      # Y-axis: embedding dimension (depth)\n",
        "plt.xlabel('Position')   # X-axis: sequence position\n",
        "plt.colorbar()           # Add a color bar to indicate value scale\n",
        "plt.show()               # Display the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKqVkl9Jlzg6"
      },
      "source": [
        "By definition these vectors align well with nearby vectors along the position axis.  \n",
        "Below, the position encoding vectors are normalized and the vector from position `1000` is compared, by dot-product, to all the others: ‚ú®üî¢"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXY-8_uEhcRD"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Normalize each positional encoding vector to unit length along the depth axis.\n",
        "# This ensures that the dot products below measure only the direction similarity, not magnitude.\n",
        "pos_encoding /= tf.norm(pos_encoding, axis=1, keepdims=True)\n",
        "\n",
        "# Select the positional encoding vector at position 1000.\n",
        "p = pos_encoding[1000]\n",
        "\n",
        "# Compute the dot product between the position-1000 vector and every other position vector.\n",
        "# tf.einsum('pd,d -> p', pos_encoding, p) computes the dot product for each position.\n",
        "dots = tf.einsum('pd,d -> p', pos_encoding, p)\n",
        "\n",
        "# Plot the dot products for all positions to visualize similarity with position 1000.\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(dots)                # Plot similarity across all positions.\n",
        "plt.ylim([0,1])               # Limit y-axis to [0, 1] for clarity.\n",
        "\n",
        "# Draw vertical lines to highlight the zoom region (positions 950 to 1050).\n",
        "plt.plot([950, 950, float('nan'), 1050, 1050],\n",
        "         [0,1,float('nan'),0,1], color='k', label='Zoom')\n",
        "plt.legend()\n",
        "\n",
        "# Plot a zoomed-in view of the dot products around position 1000.\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(dots)\n",
        "plt.xlim([950, 1050])         # Focus x-axis on positions near 1000.\n",
        "plt.ylim([0,1])               # Keep y-axis limits consistent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUknPLlVm99o"
      },
      "source": [
        "So use this to create a `PositionEmbedding` layer that looks-up a token's embedding vector and adds the position vector:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "838tmM1cm9cB"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    # Embedding layer: maps token IDs to dense vectors of size d_model.\n",
        "    # mask_zero=True ensures that padding tokens (ID=0) are masked out.\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "    # Precompute positional encodings for sequences up to length 2048.\n",
        "    # These encodings inject position information into the token embeddings.\n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    # Propagate the mask from the embedding layer.\n",
        "    # This allows downstream layers to ignore padding tokens.\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    # x: input tensor of token IDs, shape (batch_size, seq_len)\n",
        "    length = tf.shape(x)[1]  # Get the sequence length for this batch.\n",
        "    x = self.embedding(x)    # Convert token IDs to embedding vectors.\n",
        "    # Scale embeddings by sqrt(d_model) as in the original Transformer paper.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    # Add positional encoding to each token embedding.\n",
        "    # pos_encoding is sliced to match the current sequence length.\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    # Return the combined embeddings (with position information).\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpWnjwygmw-x"
      },
      "source": [
        "> Note: The [original paper](https://arxiv.org/pdf/1706.03762.pdf), section 3.4 and 5.1, uses a single tokenizer and weight matrix for both the source and target languages. This tutorial uses two separate tokenizers and weight matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfz-EaCEDfUJ"
      },
      "outputs": [],
      "source": [
        "# Create positional embedding layers for Portuguese and English tokens.\n",
        "# - vocab_size: Number of unique tokens in the tokenizer's vocabulary.\n",
        "# - d_model: Dimensionality of the embedding vectors (512, as used in the Transformer model).\n",
        "embed_pt = PositionalEmbedding(\n",
        "    vocab_size=tokenizers.pt.get_vocab_size().numpy(),  # Portuguese vocab size\n",
        "    d_model=512                                         # Embedding dimension\n",
        ")\n",
        "embed_en = PositionalEmbedding(\n",
        "    vocab_size=tokenizers.en.get_vocab_size().numpy(),  # English vocab size\n",
        "    d_model=512                                         # Embedding dimension\n",
        ")\n",
        "\n",
        "# Apply the positional embedding layers to the tokenized input sequences.\n",
        "# - pt: Tensor of Portuguese token IDs (shape: [batch_size, seq_len])\n",
        "# - en: Tensor of English token IDs (shape: [batch_size, seq_len])\n",
        "# The output is a tensor of shape [batch_size, seq_len, d_model] for each language,\n",
        "# where each token is represented by a vector that combines its learned embedding and positional encoding.\n",
        "pt_emb = embed_pt(pt)  # Portuguese token embeddings with position encoding\n",
        "en_emb = embed_en(en)  # English token embeddings with position encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fJZ_ArLELhJ"
      },
      "outputs": [],
      "source": [
        "# The _keras_mask attribute indicates which positions in the input are padding (masked out).\n",
        "# This mask is automatically generated by the Embedding layer when mask_zero=True.\n",
        "# It is a boolean tensor of shape (batch_size, sequence_length), where True means the token is not padding.\n",
        "# This mask is used by subsequent layers (like attention) to ignore padding tokens during computation.\n",
        "en_emb._keras_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE9cEBWCMKOP"
      },
      "source": [
        "### Add and normalize\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=2>Add and normalize</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/Add+Norm.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfz3WjFLTEk_"
      },
      "source": [
        "These \"Add & Norm\" blocks are scattered throughout the model. Each one joins a residual connection and runs the result through a `LayerNormalization` layer. ‚ú®‚ûïüìè\n",
        "\n",
        "The easiest way to organize the code is around these residual blocks. The following sections will define custom layer classes for each. üß©\n",
        "\n",
        "The residual \"Add & Norm\" blocks are included so that training is efficient. The residual connection provides a direct path for the gradient (and ensures that vectors are **updated** by the attention layers instead of **replaced**), while the normalization maintains a reasonable scale for the outputs. üöÄ\n",
        "\n",
        "Note: The implementations, below, use the `Add` layer to ensure that Keras masks are propagated (the `+` operator does not). üõ°Ô∏è\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJAJ2_VlPXrZ"
      },
      "source": [
        "### The base attention layer ü§ñ‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tMGOGki35KI"
      },
      "source": [
        "Attention layers are used throughout the model. These are all identical except for how the attention is configured. Each one contains a `layers.MultiHeadAttention`, a `layers.LayerNormalization` and a `layers.Add`. ü§ñ‚ú®üß†\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=2>The base attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/BaseAttention.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6chjIrOVSYp"
      },
      "source": [
        "To implement these attention layers, start with a simple base class that just contains the component layers. Each use-case will be implemented as a subclass. It's a little more code to write this way, but it keeps the intention clear. ü§ñ‚ú®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VLa5QcdPpv5"
      },
      "outputs": [],
      "source": [
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    # MultiHeadAttention layer: computes attention over input sequences.\n",
        "    # kwargs can include num_heads, key_dim, etc.\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    # LayerNormalization: normalizes the output for stable training.\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    # Add layer: adds residual connections (input + attention output).\n",
        "    self.add = tf.keras.layers.Add()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBY06TCqV2lv"
      },
      "source": [
        "#### Attention refresher ü§ñ‚ú®\n",
        "\n",
        "Before you get into the specifics of each usage, here is a quick refresher on how attention works:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BsRsq4TV5FY"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The base attention layer</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=430 src=\"https://www.tensorflow.org/images/tutorials/transformer/BaseAttention-new.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtGTy7vZ5aaT"
      },
      "source": [
        "There are two inputs: ü§îüîç\n",
        "\n",
        "1. The query sequence; the sequence being processed; the sequence doing the attending (bottom).\n",
        "2. The context sequence; the sequence being attended to (left).\n",
        "\n",
        "The output has the same shape as the query-sequence.\n",
        "\n",
        "The common comparison is that this operation is like a dictionary lookup.\n",
        "A **fuzzy**, **differentiable**, **vectorized** dictionary lookup.\n",
        "\n",
        "Here's a regular python dictionary, with 3 keys and 3 values being passed a single query.\n",
        "\n",
        "```\n",
        "d = {'color': 'blue', 'age': 22, 'type': 'pickup'}\n",
        "result = d['color']\n",
        "```\n",
        "\n",
        "- The `query`s is what you're trying to find.\n",
        "- The `key`s what sort of information the dictionary has.\n",
        "- The `value` is that information.\n",
        "\n",
        "When you look up a `query` in a regular dictionary, the dictionary finds the matching `key`, and returns its associated `value`.\n",
        "The `query` either has a matching `key` or it doesn't.\n",
        "You can imagine a **fuzzy** dictionary where the keys don't have to match perfectly.\n",
        "If you looked up `d[\"species\"]` in the dictionary above, maybe you'd want it to return `\"pickup\"` since that's the best match for the query.\n",
        "\n",
        "An attention layer does a fuzzy lookup like this, but it's not just looking for the best key.\n",
        "It combines the `values` based on how well the `query` matches each `key`.\n",
        "\n",
        "How does that work? In an attention layer the `query`, `key`, and `value` are each vectors.\n",
        "Instead of doing a hash lookup the attention layer combines the `query` and `key` vectors to determine how well they match, the \"attention score\".\n",
        "The layer returns the average across all the `values`, weighted by the \"attention scores\".\n",
        "\n",
        "Each location the query-sequence provides a `query` vector.\n",
        "The context sequence acts as the dictionary. At each location in the context sequence provides a `key` and `value` vector.\n",
        "The input vectors are not used directly, the `layers.MultiHeadAttention` layer includes `layers.Dense` layers to project the input vectors before using them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7QcPJvmv6ix"
      },
      "source": [
        "### The cross attention layer ü§ù‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8VJZqds37QC"
      },
      "source": [
        "At the literal center of the Transformer is the cross-attention layer. ü§ù‚ú® This layer connects the encoder and decoder. This layer is the most straight-forward use of attention in the model, it performs the same task as the attention block in the [NMT with attention tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention).\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The cross attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/CrossAttention.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhscgMUNUFWP"
      },
      "source": [
        "To implement this you pass the target sequence `x` as the `query` and the `context` sequence as the `key/value` when calling the `mha` layer: ü§ù‚ú®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfHVbJUWv8qp"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context):\n",
        "    # Compute multi-head attention:\n",
        "    # - query: the target sequence (x)\n",
        "    # - key/value: the context sequence (context, typically encoder output)\n",
        "    # - return_attention_scores=True: also returns the attention weights for visualization\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        key=context,\n",
        "        value=context,\n",
        "        return_attention_scores=True)\n",
        "\n",
        "    # Store the attention scores for later analysis or visualization\n",
        "    self.last_attn_scores = attn_scores\n",
        "\n",
        "    # Add the attention output to the original input (residual connection)\n",
        "    x = self.add([x, attn_output])\n",
        "    # Normalize the result for stable training\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    # Return the processed output (same shape as input x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3tJU6aTYY1X"
      },
      "source": [
        "The caricature below shows how information flows through this layer. The columns represent the weighted sum over the context sequence. üé®üîÑ\n",
        "\n",
        "For simplicity the residual connections are not shown."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBE5JNB26OjJ"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th>The cross attention layer</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=430 src=\"https://www.tensorflow.org/images/tutorials/transformer/CrossAttention-new-full.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU9MeSvzZSA-"
      },
      "source": [
        "The output length is the length of the `query` sequence, and not the length of the context `key/value` sequence. üïµÔ∏è‚Äç‚ôÇÔ∏èüîë\n",
        "\n",
        "The diagram is further simplified, below. There's no need to draw the entire \"Attention weights\" matrix. üé®\n",
        "The point is that each `query` location can see all the `key/value` pairs in the context, but no information is exchanged between the queries. üëÄüîÑ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRrB_GcyKv-4"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th>Each query sees the whole context.</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=430 src=\"https://www.tensorflow.org/images/tutorials/transformer/CrossAttention-new.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCQsj7ljKv-4"
      },
      "source": [
        "Test run it on sample inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qw1FJV5qRk79"
      },
      "outputs": [],
      "source": [
        "# Create a CrossAttention layer instance with 2 attention heads and key dimension of 512.\n",
        "# This layer will allow the decoder (English embeddings) to attend to the encoder (Portuguese embeddings).\n",
        "sample_ca = CrossAttention(num_heads=2, key_dim=512)\n",
        "\n",
        "# Print the shape of the Portuguese token embeddings with positional encoding.\n",
        "# pt_emb: Tensor of shape (batch_size, pt_seq_len, d_model)\n",
        "print(pt_emb.shape)\n",
        "\n",
        "# Print the shape of the English token embeddings with positional encoding.\n",
        "# en_emb: Tensor of shape (batch_size, en_seq_len, d_model)\n",
        "print(en_emb.shape)\n",
        "\n",
        "# Pass the English embeddings (as queries) and Portuguese embeddings (as context) to the CrossAttention layer.\n",
        "# This computes attention from each English token to all Portuguese tokens.\n",
        "# The output shape matches the input query shape: (batch_size, en_seq_len, d_model)\n",
        "print(sample_ca(en_emb, pt_emb).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6qrQxSpv34R"
      },
      "source": [
        "### The global self-attention layer ü§ñ‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-LbLRTkaTh5"
      },
      "source": [
        "This layer is responsible for processing the context sequence, and propagating information along its length: üîÑ‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlYBQX3E388Y"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The global self-attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/SelfAttention.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9j9LPJFbEkF"
      },
      "source": [
        "Since the context sequence is fixed while the translation is being generated, information is allowed to flow in both directions. üîÑ\n",
        "\n",
        "Before Transformers and self-attention, models commonly used RNNs or CNNs to do this task: ü§ñüß†"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87Rlu8N_avBF"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>Bidirectional RNNs and CNNs</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=500 src=\"https://www.tensorflow.org/images/tutorials/transformer/RNN-bidirectional.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=500 src=\"https://www.tensorflow.org/images/tutorials/transformer/CNN.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPyXM4vabhup"
      },
      "source": [
        "RNNs and CNNs have their limitations. ü§î\n",
        "\n",
        "- The RNN allows information to flow all the way across the sequence, but it passes through many processing steps to get there (limiting gradient flow). These RNN steps have to be run sequentially and so the RNN is less able to take advantage of modern parallel devices. üê¢\n",
        "- In the CNN each location can be processed in parallel, but it only provides a limited receptive field. The receptive field only grows linearly with the number of CNN layers,  You need to stack a number of Convolution layers to transmit information across the sequence ([Wavenet](https://arxiv.org/abs/1609.03499) reduces this problem by using dilated convolutions). üèóÔ∏è\n",
        "\n",
        "The global self-attention layer on the other hand lets every sequence element directly access every other sequence element, with only a few operations, and all the outputs can be computed in parallel. ‚ö°ü§ñ\n",
        "\n",
        "To implement this layer you just need to pass the target sequence, `x`, as both the `query`, and `value` arguments to the `mha` layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNqoTpn1wB3i"
      },
      "outputs": [],
      "source": [
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    # Compute multi-head self-attention:\n",
        "    # - query: the input sequence (x)\n",
        "    # - key: the input sequence (x)\n",
        "    # - value: the input sequence (x)\n",
        "    # This allows every position in the sequence to attend to every other position.\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x)\n",
        "    # Add the attention output to the original input (residual connection).\n",
        "    x = self.add([x, attn_output])\n",
        "    # Normalize the result for stable training.\n",
        "    x = self.layernorm(x)\n",
        "    # Return the processed output (same shape as input x).\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPn2D07-Jcmj"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the GlobalSelfAttention layer.\n",
        "# - num_heads=2: The attention mechanism will use 2 separate attention heads.\n",
        "# - key_dim=512: Each attention head projects the input to a 512-dimensional space.\n",
        "sample_gsa = GlobalSelfAttention(num_heads=2, key_dim=512)\n",
        "\n",
        "# Print the shape of the Portuguese token embeddings with positional encoding.\n",
        "# pt_emb: Tensor of shape (batch_size, sequence_length, embedding_dim)\n",
        "print(pt_emb.shape)\n",
        "\n",
        "# Pass the Portuguese embeddings through the GlobalSelfAttention layer.\n",
        "# This allows each token in the sequence to attend to every other token in the same sequence.\n",
        "# The output shape matches the input: (batch_size, sequence_length, embedding_dim)\n",
        "print(sample_gsa(pt_emb).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd-ga2tQfzhE"
      },
      "source": [
        "Sticking with the same style as before you could draw it like this: üé®‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1bcv9Zc6--k"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The global self-attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=330 src=\"https://www.tensorflow.org/images/tutorials/transformer/SelfAttention-new-full.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze7D0WHOe-d8"
      },
      "source": [
        "Again, the residual connections are omitted for clarity. üòä\n",
        "\n",
        "It's more compact, and just as accurate to draw it like this: üé®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imlyNt2K7RnA"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The global self-attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=500 src=\"https://www.tensorflow.org/images/tutorials/transformer/SelfAttention-new.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq4NtLymD99-"
      },
      "source": [
        "### The causal self-attention layer ‚è©ü§ñ‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VufkgF7caLze"
      },
      "source": [
        "This layer does a similar job as the global self-attention layer, for the output sequence: ‚è©ü§ñ‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KMEDiP63-hQ"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The causal self-attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/CausalSelfAttention.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AtF1HYFEOYf"
      },
      "source": [
        "This needs to be handled differently from the encoder's global self-attention layer.  \n",
        "\n",
        "Like the [text generation tutorial](https://www.tensorflow.org/text/tutorials/text_generation) and the [NMT with attention](https://www.tensorflow.org/text/tutorials/nmt_with_attention) tutorial, Transformers are an \"autoregressive\" model: They generate the text one token at a time and feed that output back to the input. To make this _efficient_, these models ensure that the output for each sequence element only depends on the previous sequence elements; the models are \"causal\". ‚è©ü§ñ‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDyn29oahHiL"
      },
      "source": [
        "A single-direction RNN is causal by definition. üïí To make a causal convolution you just need to pad the input and shift the output so that it aligns correctly (use `layers.Conv1D(padding='causal')`) . üß©‚û°Ô∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_1yd-LjhM3b"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>Causal RNNs and CNNs</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=500 src=\"https://www.tensorflow.org/images/tutorials/transformer/RNN.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=500 src=\"https://www.tensorflow.org/images/tutorials/transformer/CNN-causal.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4St1AWq9bIZg"
      },
      "source": [
        "A causal model is efficient in two ways: ‚ö°‚è©\n",
        "\n",
        "1. In training, it lets you compute loss for every location in the output sequence while executing the model just once. üèãÔ∏è‚Äç‚ôÇÔ∏è\n",
        "2. During inference, for each new token generated you only need to calculate its outputs, the outputs for the previous sequence elements can be reused. üîÑ\n",
        "  - For an RNN you just need the RNN-state to account for previous computations (pass `return_state=True` to the RNN layer's constructor). üß†\n",
        "  - For a CNN you would need to follow the approach of [Fast Wavenet](https://arxiv.org/abs/1611.09482) üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLYfIa8eiYgk"
      },
      "source": [
        "To build a causal self-attention layer, you need to use an appropriate mask when computing the attention scores and summing the attention `value`s. üõ°Ô∏è‚ú®\n",
        "\n",
        "This is taken care of automatically if you pass `use_causal_mask = True` to the `MultiHeadAttention` layer when you call it: ü§ñ‚è©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MMQ-AfKD99_"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    # Compute multi-head self-attention with a causal mask:\n",
        "    # - query: the input sequence (x)\n",
        "    # - key: the input sequence (x)\n",
        "    # - value: the input sequence (x)\n",
        "    # - use_causal_mask=True: ensures that each position can only attend to previous positions (not future ones)\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask=True)\n",
        "    # Add the attention output to the original input (residual connection)\n",
        "    x = self.add([x, attn_output])\n",
        "    # Normalize the result for stable training\n",
        "    x = self.layernorm(x)\n",
        "    # Return the processed output (same shape as input x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5oumNdAjI-D"
      },
      "source": [
        "The causal mask ensures that each location only has access to the locations that come before it:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFJy5L1U8TBt"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The causal self-attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=330 src=\"https://www.tensorflow.org/images/tutorials/transformer/CausalSelfAttention-new-full.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWX0RNnFjaCj"
      },
      "source": [
        "Again, the residual connections are omitted for simplicity.\n",
        "\n",
        "The more compact representation of this layer would be:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C9qVfvh8-jp"
      },
      "source": [
        "<table>\n",
        "</tr>\n",
        "  <th colspan=1>The causal self-attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=430 src=\"https://www.tensorflow.org/images/tutorials/transformer/CausalSelfAttention-new.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQBhYEZ2jfrX"
      },
      "source": [
        "Test out the layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4dQuzvlD99_"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the CausalSelfAttention layer.\n",
        "# - num_heads=2: The attention mechanism will use 2 separate attention heads.\n",
        "# - key_dim=512: Each attention head projects the input to a 512-dimensional space.\n",
        "sample_csa = CausalSelfAttention(num_heads=2, key_dim=512)\n",
        "\n",
        "# Print the shape of the English token embeddings with positional encoding.\n",
        "# en_emb: Tensor of shape (batch_size, sequence_length, embedding_dim)\n",
        "print(en_emb.shape)\n",
        "\n",
        "# Pass the English embeddings through the CausalSelfAttention layer.\n",
        "# This allows each token in the sequence to attend only to previous tokens (not future ones),\n",
        "# enforcing causality for autoregressive decoding.\n",
        "# The output shape matches the input: (batch_size, sequence_length, embedding_dim)\n",
        "print(sample_csa(en_emb).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-IPCEkajleb"
      },
      "source": [
        "The output for early sequence elements doesn't depend on later elements, so it shouldn't matter if you trim elements before or after applying the layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwKlheQ-WVxl"
      },
      "outputs": [],
      "source": [
        "# Compute the output of the CausalSelfAttention layer on the first 3 tokens of each sequence.\n",
        "# - en[:, :3]: Selects the first 3 tokens from each sequence in the English input batch.\n",
        "# - embed_en(en[:, :3]): Applies the positional embedding to these tokens.\n",
        "# - sample_csa(...): Passes the embedded tokens through the causal self-attention layer.\n",
        "out1 = sample_csa(embed_en(en[:, :3]))\n",
        "\n",
        "# Compute the output of the CausalSelfAttention layer on the full sequence,\n",
        "# then select only the first 3 tokens from the output.\n",
        "# - embed_en(en): Applies positional embedding to the full English input batch.\n",
        "# - sample_csa(...): Passes the full embedded sequence through the causal self-attention layer.\n",
        "# - ...[:, :3]: Selects the first 3 tokens from the output sequence.\n",
        "out2 = sample_csa(embed_en(en))[:, :3]\n",
        "\n",
        "# Calculate the maximum absolute difference between the two outputs.\n",
        "# - abs(out1 - out2): Element-wise absolute difference between the two tensors.\n",
        "# - tf.reduce_max(...): Finds the maximum value in the difference tensor.\n",
        "# - .numpy(): Converts the result to a NumPy scalar for display.\n",
        "tf.reduce_max(abs(out1 - out2)).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOVv38ynuBW-"
      },
      "source": [
        "Note: When using Keras masks, the output values at invalid locations are not well defined. So the above may not hold for masked regions. ‚ö†Ô∏èü§ñ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLjScSWQv9M5"
      },
      "source": [
        "### The feed forward network ‚ö°üß†"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz0HBopX_VdU"
      },
      "source": [
        "The transformer also includes this point-wise feed-forward network in both the encoder and decoder: ‚ö°üß†"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDHMWoZ94AUU"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The feed forward network</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/FeedForward.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Yb-IV0Nlzd0"
      },
      "source": [
        "The network consists of two linear layers (`tf.keras.layers.Dense`) with a ReLU activation in-between, and a dropout layer. As with the attention layers the code here also includes the residual connection and normalization: ‚ö°üß†‚ûïüìè"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAYLeu0uwXYK"
      },
      "outputs": [],
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    # Sequential block: two dense layers with ReLU and dropout in between.\n",
        "    # - First Dense: expands to dff units (hidden size), ReLU activation.\n",
        "    # - Second Dense: projects back to d_model (original embedding size).\n",
        "    # - Dropout: regularizes the output to prevent overfitting.\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),   # Hidden layer with ReLU\n",
        "      tf.keras.layers.Dense(d_model),                  # Output layer, restores original size\n",
        "      tf.keras.layers.Dropout(dropout_rate)            # Dropout for regularization\n",
        "    ])\n",
        "    # Add layer for residual connection (input + output of FFN)\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    # LayerNormalization to stabilize training and scale outputs\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    # Apply the feed-forward network to the input\n",
        "    # Add the input (residual connection) to the output of the FFN\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    # Normalize the result for stable training\n",
        "    x = self.layer_norm(x)\n",
        "    # Return the processed tensor\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQBlOVQU_hUt"
      },
      "source": [
        "Test the layer, the output is the same shape as the input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-Y8Yqi1_hUt"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the FeedForward layer.\n",
        "# - d_model=512: The input and output dimensionality of the layer (matches embedding size).\n",
        "# - dff=2048: The hidden layer size inside the feed-forward network.\n",
        "sample_ffn = FeedForward(512, 2048)\n",
        "\n",
        "# Print the shape of the English token embeddings with positional encoding.\n",
        "# en_emb: Tensor of shape (batch_size, sequence_length, embedding_dim)\n",
        "print(en_emb.shape)\n",
        "\n",
        "# Pass the English embeddings through the FeedForward layer.\n",
        "# The output shape matches the input: (batch_size, sequence_length, embedding_dim)\n",
        "# This layer applies two dense layers with ReLU and dropout, then adds a residual connection and normalizes the result.\n",
        "print(sample_ffn(en_emb).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFv-FNYUmvpn"
      },
      "source": [
        "### The encoder layer üß©‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk-DAL2xv4PZ"
      },
      "source": [
        "The encoder contains a stack of `N` encoder layers. Where each `EncoderLayer` contains a `GlobalSelfAttention` and `FeedForward` layer: üß©‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgPaE3f44Cgh"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The encoder layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/EncoderLayer.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kRUT__Ly9HH"
      },
      "source": [
        "Here is the definition of the `EncoderLayer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncyS-Ms3i2x_"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    # GlobalSelfAttention: allows each token in the input sequence to attend to every other token.\n",
        "    # - num_heads: Number of attention heads.\n",
        "    # - key_dim: Dimensionality of each attention head (usually matches d_model).\n",
        "    # - dropout: Dropout rate for regularization.\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    # FeedForward: point-wise feed-forward network applied to each position.\n",
        "    # - d_model: Input/output dimensionality (matches embedding size).\n",
        "    # - dff: Hidden layer size inside the feed-forward network.\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    # Apply global self-attention to the input sequence.\n",
        "    # This propagates information along the sequence.\n",
        "    x = self.self_attention(x)\n",
        "    # Apply the feed-forward network to each position in the sequence.\n",
        "    x = self.ffn(x)\n",
        "    # Return the processed sequence (same shape as input).\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeXHMUlb6q6F"
      },
      "source": [
        "And a quick test, the output will have the same shape as the input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzZRXdO0mI48"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the EncoderLayer.\n",
        "# - d_model=512: The dimensionality of the input and output vectors (embedding size).\n",
        "# - num_heads=8: Number of attention heads in the multi-head attention mechanism.\n",
        "# - dff=2048: The hidden layer size inside the feed-forward network.\n",
        "sample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)\n",
        "\n",
        "# Print the shape of the Portuguese token embeddings with positional encoding.\n",
        "# pt_emb: Tensor of shape (batch_size, sequence_length, embedding_dim)\n",
        "print(pt_emb.shape)\n",
        "\n",
        "# Pass the Portuguese embeddings through the EncoderLayer.\n",
        "# The EncoderLayer applies:\n",
        "#   1. Global self-attention: Each token attends to every other token in the sequence.\n",
        "#   2. Feed-forward network: Point-wise transformation of each token embedding.\n",
        "# The output shape matches the input: (batch_size, sequence_length, embedding_dim)\n",
        "print(sample_encoder_layer(pt_emb).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE1H51Ajm0q1"
      },
      "source": [
        "### The encoder üß©‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fym9ah11ykMd"
      },
      "source": [
        "Next build the encoder. üèóÔ∏è‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXI2B-Ad4ETO"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The encoder</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/Encoder.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA6sVo5rlzd3"
      },
      "source": [
        "The encoder consists of: üèóÔ∏è‚ú®\n",
        "\n",
        "- A `PositionalEmbedding` layer at the input. üß©üî¢\n",
        "- A stack of `EncoderLayer` layers. üß±üß±üß±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpEox7gJ8FCI"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    # Store the model dimension and number of layers for reference\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # PositionalEmbedding: Converts token IDs to embeddings and adds positional encoding\n",
        "    # - vocab_size: Number of unique tokens in the vocabulary\n",
        "    # - d_model: Dimensionality of the embedding vectors\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, d_model=d_model)\n",
        "\n",
        "    # Stack of EncoderLayer instances\n",
        "    # Each EncoderLayer contains:\n",
        "    #   - GlobalSelfAttention: lets each token attend to every other token in the sequence\n",
        "    #   - FeedForward: point-wise feed-forward network\n",
        "    # - num_layers: Number of layers to stack\n",
        "    # - d_model: Embedding dimension\n",
        "    # - num_heads: Number of attention heads\n",
        "    # - dff: Hidden layer size in the feed-forward network\n",
        "    # - dropout_rate: Dropout rate for regularization\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    # Dropout layer applied after positional embedding for regularization\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    # Input x: token IDs of shape (batch_size, seq_len)\n",
        "    # Convert token IDs to embeddings and add positional encoding\n",
        "    x = self.pos_embedding(x)  # Shape: (batch_size, seq_len, d_model)\n",
        "\n",
        "    # Apply dropout to the embeddings\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    # Pass the embeddings through each EncoderLayer in the stack\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "\n",
        "    # Output: processed sequence of shape (batch_size, seq_len, d_model)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "texobMBHLBEU"
      },
      "source": [
        "Test the encoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDPXTvYgJH8s"
      },
      "outputs": [],
      "source": [
        "# Instantiate the encoder with the specified configuration:\n",
        "# - num_layers=4: The encoder will have 4 stacked EncoderLayer blocks.\n",
        "# - d_model=512: Each token embedding and hidden state will have 512 dimensions.\n",
        "# - num_heads=8: Multi-head attention will use 8 parallel attention heads.\n",
        "# - dff=2048: The feed-forward network inside each EncoderLayer will expand to 2048 units before projecting back to d_model.\n",
        "# - vocab_size=8500: The vocabulary size for the input tokens (number of unique tokens).\n",
        "sample_encoder = Encoder(\n",
        "    num_layers=4,      # Number of encoder layers to stack\n",
        "    d_model=512,       # Dimensionality of embeddings and hidden states\n",
        "    num_heads=8,       # Number of attention heads in MultiHeadAttention\n",
        "    dff=2048,          # Hidden layer size in the feed-forward network\n",
        "    vocab_size=8500    # Size of the input vocabulary\n",
        ")\n",
        "\n",
        "# Pass the Portuguese token IDs (pt) through the encoder.\n",
        "# - pt: Tensor of shape (batch_size, input_seq_len), containing token IDs for each input sentence.\n",
        "# - training=False: Indicates that the encoder is running in inference mode (no dropout applied).\n",
        "# The encoder will:\n",
        "#   1. Embed the input tokens and add positional encoding.\n",
        "#   2. Apply dropout (skipped if training=False).\n",
        "#   3. Pass the embeddings through 4 stacked EncoderLayer blocks.\n",
        "#   4. Each EncoderLayer applies global self-attention and a feed-forward network.\n",
        "# The output is a tensor of shape (batch_size, input_seq_len, d_model), representing the encoded input sequence.\n",
        "sample_encoder_output = sample_encoder(pt, training=False)\n",
        "\n",
        "# Print the shape of the input token IDs tensor.\n",
        "print(pt.shape)  # Shape: (batch_size, input_seq_len)\n",
        "\n",
        "# Print the shape of the encoder output tensor.\n",
        "# The output shape should be (batch_size, input_seq_len, d_model), where:\n",
        "#   - batch_size: Number of input sequences in the batch\n",
        "#   - input_seq_len: Length of each input sequence (number of tokens)\n",
        "#   - d_model: Dimensionality of the encoder output vectors (512)\n",
        "print(sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LO_48Owmx_o"
      },
      "source": [
        "### The decoder layer üß©‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGxm57u6E4g2"
      },
      "source": [
        "The decoder's stack is slightly more complex, with each `DecoderLayer` containing a `CausalSelfAttention` ‚è©ü§ñ, a `CrossAttention` ü§ù‚ú®, and a `FeedForward` ‚ö°üß† layer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZYER7rC4FmI"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The decoder layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/DecoderLayer.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SoX0-vd1hue"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    # CausalSelfAttention: allows each token in the output sequence to attend only to previous tokens (not future ones).\n",
        "    # - num_heads: Number of attention heads.\n",
        "    # - key_dim: Dimensionality of each attention head (usually matches d_model).\n",
        "    # - dropout: Dropout rate for regularization.\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    # CrossAttention: allows each token in the output sequence to attend to all tokens in the encoder output (context).\n",
        "    # - num_heads: Number of attention heads.\n",
        "    # - key_dim: Dimensionality of each attention head (usually matches d_model).\n",
        "    # - dropout: Dropout rate for regularization.\n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    # FeedForward: point-wise feed-forward network applied to each position in the sequence.\n",
        "    # - d_model: Input/output dimensionality (matches embedding size).\n",
        "    # - dff: Hidden layer size inside the feed-forward network.\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x, context):\n",
        "    # Apply causal self-attention to the input sequence (decoder input).\n",
        "    # This enforces autoregressive decoding by masking future tokens.\n",
        "    x = self.causal_self_attention(x=x)\n",
        "\n",
        "    # Apply cross-attention, allowing the decoder to attend to the encoder output (context).\n",
        "    x = self.cross_attention(x=x, context=context)\n",
        "\n",
        "    # Cache the last attention scores from the cross-attention layer for visualization or analysis.\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "    # Apply the feed-forward network to each position in the sequence.\n",
        "    # The output shape matches the input: (batch_size, seq_len, d_model).\n",
        "    x = self.ffn(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6T3RSR_6nJX"
      },
      "source": [
        "Test the decoder layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ne2Bqx8k71l0"
      },
      "outputs": [],
      "source": [
        "# Instantiate a DecoderLayer with the specified configuration:\n",
        "# - d_model=512: Dimensionality of the input and output vectors (embedding size).\n",
        "# - num_heads=8: Number of attention heads in the multi-head attention mechanism.\n",
        "# - dff=2048: Hidden layer size inside the feed-forward network.\n",
        "sample_decoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=2048)\n",
        "\n",
        "# Pass the English token embeddings (en_emb) and Portuguese token embeddings (pt_emb) to the decoder layer.\n",
        "# - x=en_emb: The input to the decoder (target sequence embeddings with positional encoding).\n",
        "# - context=pt_emb: The encoder output (source sequence embeddings with positional encoding).\n",
        "# The DecoderLayer applies:\n",
        "#   1. Causal self-attention: Each token in the target sequence attends only to previous tokens (autoregressive).\n",
        "#   2. Cross-attention: Each token in the target sequence attends to all tokens in the source sequence (encoder output).\n",
        "#   3. Feed-forward network: Point-wise transformation of each token embedding.\n",
        "# The output shape matches the input: (batch_size, seq_len, d_model)\n",
        "sample_decoder_layer_output = sample_decoder_layer(\n",
        "    x=en_emb, context=pt_emb)\n",
        "\n",
        "# Print the shape of the English token embeddings (input to the decoder).\n",
        "print(en_emb.shape)  # Shape: (batch_size, target_seq_len, d_model)\n",
        "\n",
        "# Print the shape of the Portuguese token embeddings (encoder output/context).\n",
        "print(pt_emb.shape)  # Shape: (batch_size, source_seq_len, d_model)\n",
        "\n",
        "# Print the shape of the decoder layer output.\n",
        "# The output shape should be (batch_size, target_seq_len, d_model), where:\n",
        "#   - batch_size: Number of sequences in the batch\n",
        "#   - target_seq_len: Length of each target sequence (number of tokens)\n",
        "#   - d_model: Dimensionality of the decoder output vectors (512)\n",
        "print(sample_decoder_layer_output.shape)  # `(batch_size, seq_len, d_model)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-uO6ls8m2O5"
      },
      "source": [
        "### The decoder üß©‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgj3c0TVF3Pb"
      },
      "source": [
        "Similar to the `Encoder`, the `Decoder` consists of a `PositionalEmbedding`, and a stack of `DecoderLayer`s:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADGss2nT4Gt-"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The embedding and positional encoding layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/Decoder.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q49vtv5lzd3"
      },
      "source": [
        "\n",
        "Define the decoder by extending `tf.keras.layers.Layer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5_d5-PLQXwY"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    # Store model hyperparameters for reference\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # PositionalEmbedding: Converts token IDs to embeddings and adds positional encoding\n",
        "    # - vocab_size: Number of unique tokens in the vocabulary\n",
        "    # - d_model: Dimensionality of the embedding vectors\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
        "                                             d_model=d_model)\n",
        "    # Dropout layer for regularization after embedding\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    # Stack of DecoderLayer instances\n",
        "    # Each DecoderLayer contains:\n",
        "    #   - CausalSelfAttention: lets each token attend only to previous tokens in the sequence\n",
        "    #   - CrossAttention: lets each token attend to all tokens in the encoder output\n",
        "    #   - FeedForward: point-wise feed-forward network\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    # Will store the attention scores from the last DecoderLayer for visualization/analysis\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context):\n",
        "    # x: token IDs of shape (batch_size, target_seq_len)\n",
        "    # context: encoder output, shape (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    # Convert token IDs to embeddings and add positional encoding\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    # Apply dropout to the embeddings\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    # Pass the embeddings through each DecoderLayer in the stack\n",
        "    # Each layer applies causal self-attention, cross-attention, and feed-forward network\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context)\n",
        "\n",
        "    # Cache the last attention scores from the final DecoderLayer for visualization/analysis\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "    # Output: processed sequence of shape (batch_size, target_seq_len, d_model)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eALcB--YMmLf"
      },
      "source": [
        "Test the decoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyHdG_jWPgKu"
      },
      "outputs": [],
      "source": [
        "# Instantiate the decoder with the specified configuration:\n",
        "# - num_layers=4: The decoder will have 4 stacked DecoderLayer blocks.\n",
        "# - d_model=512: Each token embedding and hidden state will have 512 dimensions.\n",
        "# - num_heads=8: Multi-head attention will use 8 parallel attention heads.\n",
        "# - dff=2048: The feed-forward network inside each DecoderLayer will expand to 2048 units before projecting back to d_model.\n",
        "# - vocab_size=8000: The vocabulary size for the target language tokens.\n",
        "sample_decoder = Decoder(\n",
        "    num_layers=4,      # Number of decoder layers to stack\n",
        "    d_model=512,       # Dimensionality of embeddings and hidden states\n",
        "    num_heads=8,       # Number of attention heads in MultiHeadAttention\n",
        "    dff=2048,          # Hidden layer size in the feed-forward network\n",
        "    vocab_size=8000    # Size of the target vocabulary\n",
        ")\n",
        "\n",
        "# Pass the English token IDs (en) and Portuguese embeddings (pt_emb) to the decoder.\n",
        "# - x=en: Tensor of shape (batch_size, target_seq_len), containing token IDs for each target sentence.\n",
        "# - context=pt_emb: Tensor of shape (batch_size, source_seq_len, d_model), containing encoder output embeddings.\n",
        "# The decoder will:\n",
        "#   1. Embed the target tokens and add positional encoding.\n",
        "#   2. Apply dropout (if training).\n",
        "#   3. Pass the embeddings through 4 stacked DecoderLayer blocks.\n",
        "#   4. Each DecoderLayer applies causal self-attention, cross-attention, and a feed-forward network.\n",
        "# The output is a tensor of shape (batch_size, target_seq_len, d_model), representing the decoded sequence.\n",
        "output = sample_decoder(\n",
        "    x=en,         # Target token IDs\n",
        "    context=pt_emb  # Encoder output embeddings\n",
        ")\n",
        "\n",
        "# Print the shapes of the input and output tensors for verification.\n",
        "print(en.shape)        # Shape: (batch_size, target_seq_len) - input token IDs\n",
        "print(pt_emb.shape)    # Shape: (batch_size, source_seq_len, d_model) - encoder output\n",
        "print(output.shape)    # Shape: (batch_size, target_seq_len, d_model) - decoder output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioJ4XJAUAReI"
      },
      "outputs": [],
      "source": [
        "# The attention scores from the last DecoderLayer in the sample_decoder.\n",
        "# This tensor contains the cross-attention weights, showing how much each target token (in the output sequence)\n",
        "# attends to each input token (in the encoder output) for each attention head and batch.\n",
        "# The shape of the tensor is (batch_size, num_heads, target_seq_len, input_seq_len):\n",
        "#   - batch_size: Number of sequences in the batch.\n",
        "#   - num_heads: Number of attention heads in the decoder.\n",
        "#   - target_seq_len: Length of the output (target) sequence.\n",
        "#   - input_seq_len: Length of the input (source) sequence.\n",
        "sample_decoder.last_attn_scores.shape  # (batch, heads, target_seq, input_seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3uvMP5vNuOV"
      },
      "source": [
        "Having created the Transformer encoder and decoder, it's time to build the Transformer model and train it. üöÄü§ñ‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y54xnJnuYgJ7"
      },
      "source": [
        "## The Transformer üöÄü§ñ‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSi8vBN1lzd4"
      },
      "source": [
        "You now have `Encoder` üß© and `Decoder` üß©. To complete the `Transformer` model üöÄü§ñ‚ú®, you need to put them together and add a final linear (`Dense`) layer üß† which converts the resulting vector at each location into output token probabilities üî¢.\n",
        "\n",
        "The output of the decoder is the input to this final linear layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46nL2X_84Iud"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The transformer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trHHo2z_LC-u"
      },
      "source": [
        "A `Transformer` with one layer in both the `Encoder` and `Decoder` looks almost exactly like the model from the [RNN+attention tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention). ü§ñ‚ú® A multi-layer Transformer has more layers, but is fundamentally doing the same thing. üß©üîÑ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09kxrwiaBB36"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>A 1-layer transformer</th>\n",
        "  <th colspan=1>A 4-layer transformer</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-1layer-compact.png\"/>\n",
        "  </td>\n",
        "  <td rowspan=3>\n",
        "   <img width=330 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-4layer-compact.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th colspan=1>The RNN+Attention model</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/RNN+attention-compact.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5vSbJ_gKx7C"
      },
      "source": [
        "Create the `Transformer` by extending `tf.keras.Model` ü§ñ‚ú®:\n",
        "\n",
        "> Note: The [original paper](https://arxiv.org/pdf/1706.03762.pdf), section 3.4, shares the weight matrix between the embedding layer and the final linear layer. To keep things simple, this tutorial uses two separate weight matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PED3bIpOYkBu"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    # Encoder: Processes the input sequence and produces a context representation.\n",
        "    # - num_layers: Number of stacked EncoderLayer blocks.\n",
        "    # - d_model: Dimensionality of embeddings and hidden states.\n",
        "    # - num_heads: Number of attention heads in MultiHeadAttention.\n",
        "    # - dff: Hidden layer size in the feed-forward network.\n",
        "    # - vocab_size: Size of the input vocabulary.\n",
        "    # - dropout_rate: Dropout rate for regularization.\n",
        "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=input_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    # Decoder: Processes the target sequence and attends to the encoder output.\n",
        "    # - num_layers: Number of stacked DecoderLayer blocks.\n",
        "    # - d_model: Dimensionality of embeddings and hidden states.\n",
        "    # - num_heads: Number of attention heads in MultiHeadAttention.\n",
        "    # - dff: Hidden layer size in the feed-forward network.\n",
        "    # - vocab_size: Size of the target vocabulary.\n",
        "    # - dropout_rate: Dropout rate for regularization.\n",
        "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    # Final linear layer: Projects decoder outputs to logits for each target token.\n",
        "    # - target_vocab_size: Number of possible output tokens.\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # The call method defines the forward pass of the model.\n",
        "    # Inputs:\n",
        "    #   - inputs: Tuple (context, x)\n",
        "    #     - context: Input token IDs (source sequence), shape (batch_size, context_len)\n",
        "    #     - x: Target token IDs (target sequence), shape (batch_size, target_len)\n",
        "    context, x  = inputs\n",
        "\n",
        "    # Pass the input tokens through the encoder to get the context representation.\n",
        "    # Output shape: (batch_size, context_len, d_model)\n",
        "    context = self.encoder(context)\n",
        "\n",
        "    # Pass the target tokens and context through the decoder.\n",
        "    # Output shape: (batch_size, target_len, d_model)\n",
        "    x = self.decoder(x, context)\n",
        "\n",
        "    # Project the decoder output to logits for each token in the target vocabulary.\n",
        "    # Output shape: (batch_size, target_len, target_vocab_size)\n",
        "    logits = self.final_layer(x)\n",
        "\n",
        "    try:\n",
        "      # Remove the Keras mask from logits to avoid affecting loss/metrics calculations.\n",
        "      # This is a workaround for a known issue (b/250038731).\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    # Return the logits (predicted token probabilities for each position).\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsINyf1VEQLC"
      },
      "source": [
        "### Hyperparameters ‚öôÔ∏è‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjwMq_ixlzd5"
      },
      "source": [
        "To keep this example small and relatively fast, the number of layers (`num_layers`), the dimensionality of the embeddings (`d_model`), and the internal dimensionality of the `FeedForward` layer (`dff`) have been reduced. ‚ö°‚ú®\n",
        "\n",
        "The base model described in the original Transformer paper used `num_layers=6`, `d_model=512`, and `dff=2048`. üìÑü§ñ\n",
        "\n",
        "The number of self-attention heads remains the same (`num_heads=8`). üß†üîÑ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzyo6KDfVyhl"
      },
      "outputs": [],
      "source": [
        "# Set the hyperparameters for the Transformer model.\n",
        "# These control the size and complexity of the model.\n",
        "\n",
        "num_layers = 4        # Number of encoder/decoder layers to stack in the Transformer.\n",
        "d_model = 128         # Dimensionality of the embedding vectors and hidden states.\n",
        "dff = 512             # Dimensionality of the feed-forward network's hidden layer.\n",
        "num_heads = 8         # Number of attention heads in the MultiHeadAttention layers.\n",
        "dropout_rate = 0.1    # Dropout rate for regularization to prevent overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g08YOE-zHRqY"
      },
      "source": [
        "### Try it out üöÄü§ñ‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYbXDEhhlzd6"
      },
      "source": [
        "Instantiate the `Transformer` model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiysUa--4tOU"
      },
      "outputs": [],
      "source": [
        "# Instantiate the Transformer model with the specified hyperparameters and vocabulary sizes.\n",
        "# - num_layers: Number of encoder and decoder layers to stack in the Transformer.\n",
        "# - d_model: Dimensionality of the embedding vectors and hidden states.\n",
        "# - num_heads: Number of attention heads in the MultiHeadAttention layers.\n",
        "# - dff: Dimensionality of the feed-forward network's hidden layer.\n",
        "# - input_vocab_size: Size of the input vocabulary (Portuguese), obtained from the tokenizer.\n",
        "# - target_vocab_size: Size of the target vocabulary (English), obtained from the tokenizer.\n",
        "# - dropout_rate: Dropout rate for regularization to prevent overfitting.\n",
        "transformer = Transformer(\n",
        "    num_layers=num_layers,                          # Number of layers in encoder/decoder\n",
        "    d_model=d_model,                                # Embedding and hidden state size\n",
        "    num_heads=num_heads,                            # Number of attention heads\n",
        "    dff=dff,                                        # Feed-forward network hidden size\n",
        "    input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),   # Portuguese vocab size\n",
        "    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),  # English vocab size\n",
        "    dropout_rate=dropout_rate                       # Dropout rate for regularization\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qbw3CYn2tQQx"
      },
      "source": [
        "Test it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8eO85hpFHmE"
      },
      "outputs": [],
      "source": [
        "# Pass the Portuguese and English token IDs through the Transformer model.\n",
        "# - pt: Tensor of shape (batch_size, input_seq_len), containing token IDs for the source (Portuguese) sentences.\n",
        "# - en: Tensor of shape (batch_size, target_seq_len), containing token IDs for the target (English) sentences.\n",
        "# The transformer model will:\n",
        "#   1. Encode the Portuguese input sequence using the encoder stack.\n",
        "#   2. Decode the English target sequence using the decoder stack, attending to the encoder output.\n",
        "#   3. Project the decoder output to logits for each token in the target vocabulary.\n",
        "output = transformer((pt, en))\n",
        "\n",
        "# Print the shape of the English token IDs tensor (target sequence).\n",
        "# This shows the batch size and target sequence length.\n",
        "print(en.shape)\n",
        "\n",
        "# Print the shape of the Portuguese token IDs tensor (source sequence).\n",
        "# This shows the batch size and input sequence length.\n",
        "print(pt.shape)\n",
        "\n",
        "# Print the shape of the output tensor from the Transformer model.\n",
        "# The output shape is (batch_size, target_seq_len, target_vocab_size), representing the predicted logits for each token position in the target sequence.\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olTLrK8pAcLd"
      },
      "outputs": [],
      "source": [
        "# Retrieve the cross-attention scores from the last DecoderLayer in the Transformer.\n",
        "# - transformer.decoder.dec_layers[-1]: Accesses the last DecoderLayer in the decoder stack.\n",
        "# - last_attn_scores: Stores the cross-attention weights from the most recent forward pass.\n",
        "#   These scores indicate how much each target token (in the output sequence) attends to each input token (in the encoder output),\n",
        "#   for every attention head and batch in the input.\n",
        "attn_scores = transformer.decoder.dec_layers[-1].last_attn_scores\n",
        "\n",
        "# Print the shape of the attention scores tensor.\n",
        "# The shape is (batch_size, num_heads, target_seq_len, input_seq_len), where:\n",
        "#   - batch_size: Number of sequences in the batch.\n",
        "#   - num_heads: Number of attention heads in the decoder.\n",
        "#   - target_seq_len: Length of the output (target) sequence.\n",
        "#   - input_seq_len: Length of the input (source) sequence.\n",
        "print(attn_scores.shape)  # (batch, heads, target_seq, input_seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jTvJsXquaHW"
      },
      "source": [
        "Print the summary of the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRnl4URhllDY"
      },
      "outputs": [],
      "source": [
        "# Install the pydot package, which is used for generating and visualizing graphs in Python.\n",
        "# pydot is often used together with Graphviz for plotting model architectures, especially in TensorFlow and Keras.\n",
        "# The commented-out line shows how to install Graphviz via apt (for Linux systems), which is required for rendering graphs.\n",
        "# For most use cases in Jupyter, installing pydot via pip is sufficient if Graphviz is already available on your system.\n",
        "!pip install pydot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsUPhlfEtOjn"
      },
      "outputs": [],
      "source": [
        "# Import the plot_model utility from TensorFlow Keras.\n",
        "# This function generates a visual diagram of the model architecture.\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Visualize the architecture of the 'transformer' model.\n",
        "# - transformer: The model instance to visualize.\n",
        "# - show_shapes=True: Display the shape of the input and output tensors for each layer.\n",
        "# - dpi=64: Set the resolution of the generated image (dots per inch).\n",
        "# - rankdir='LR': Arrange the diagram from left to right (horizontal layout).\n",
        "plot_model(transformer, show_shapes=True, dpi=64, rankdir='LR')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfoBfC2oQtEy"
      },
      "source": [
        "## Training üèãÔ∏è‚Äç‚ôÇÔ∏è‚ú®\n",
        "\n",
        "It's time to prepare the model and start training it. üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYEGhEOtzn5W"
      },
      "source": [
        "### Set up the optimizer ‚ö°üß†‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL4G5bS6lzd5"
      },
      "source": [
        "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the original Transformer [paper](https://arxiv.org/abs/1706.03762).\n",
        "\n",
        "$$\\Large{lrate = d_{model}^{-0.5} * \\min(step{\\_}num^{-0.5}, step{\\_}num \\cdot warmup{\\_}steps^{-1.5})}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYQdOO1axwEI"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    # Store the model dimensionality (d_model) as a float tensor.\n",
        "    # This is used to scale the learning rate according to the formula.\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    # Store the number of warmup steps.\n",
        "    # During warmup, the learning rate increases linearly.\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    # Convert the current training step to float for computation.\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "\n",
        "    # Compute the first term: inverse square root of the step number.\n",
        "    # This causes the learning rate to decrease as training progresses.\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "\n",
        "    # Compute the second term: step number multiplied by the inverse 1.5 power of warmup_steps.\n",
        "    # This causes the learning rate to increase linearly during the warmup period.\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    # The learning rate is scaled by the inverse square root of d_model,\n",
        "    # and is the minimum of the two terms above.\n",
        "    # This implements the learning rate schedule from the original Transformer paper:\n",
        "    # lrate = d_model^{-0.5} * min(step_num^{-0.5}, step_num * warmup_steps^{-1.5})\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzXq5LWgRN63"
      },
      "source": [
        "Instantiate the optimizer (in this example it's `tf.keras.optimizers.Adam`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r4scdulztRx"
      },
      "outputs": [],
      "source": [
        "# Create a custom learning rate schedule using the formula from the original Transformer paper.\n",
        "# - d_model: Dimensionality of the model's embeddings and hidden states.\n",
        "# The learning rate will increase linearly for the first 'warmup_steps' training steps,\n",
        "# then decay proportionally to the inverse square root of the step number.\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "# Instantiate the Adam optimizer for training the Transformer model.\n",
        "# - learning_rate: Uses the custom schedule defined above.\n",
        "# - beta_1=0.9: Exponential decay rate for the first moment estimates (default for Adam).\n",
        "# - beta_2=0.98: Exponential decay rate for the second moment estimates (slightly higher than default).\n",
        "# - epsilon=1e-9: Small constant to prevent division by zero in the optimizer's update step.\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate,      # Custom learning rate schedule\n",
        "    beta_1=0.9,         # Decay rate for first moment estimates\n",
        "    beta_2=0.98,        # Decay rate for second moment estimates\n",
        "    epsilon=1e-9        # Numerical stability constant\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTb2S4RnQ8DU"
      },
      "source": [
        "Test the custom learning rate scheduler:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xij3MwYVRAAS"
      },
      "outputs": [],
      "source": [
        "# Plot the learning rate schedule over training steps.\n",
        "# - learning_rate: Custom learning rate scheduler (instance of CustomSchedule).\n",
        "# - tf.range(40000, dtype=tf.float32): Generates a tensor of training steps from 0 to 39,999.\n",
        "# - learning_rate(...): Computes the learning rate for each training step.\n",
        "# - plt.plot(...): Plots the computed learning rates against training steps.\n",
        "plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))\n",
        "\n",
        "# Set the label for the y-axis to indicate it represents the learning rate.\n",
        "plt.ylabel('Learning Rate')\n",
        "\n",
        "# Set the label for the x-axis to indicate it represents the training step number.\n",
        "plt.xlabel('Train Step')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgkDE7hzo8r5"
      },
      "source": [
        "### Set up the loss and metrics ‚ö°üìè‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6y7rNP5lzd6"
      },
      "source": [
        "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss. Use the cross-entropy loss function (`tf.keras.losses.SparseCategoricalCrossentropy`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67oqVHiT0Eiu"
      },
      "outputs": [],
      "source": [
        "def masked_loss(label, pred):\n",
        "  # Create a mask to ignore padding tokens (assumed to be 0)\n",
        "  mask = label != 0\n",
        "\n",
        "  # Define the loss function: SparseCategoricalCrossentropy\n",
        "  # - from_logits=True: pred contains raw logits, not probabilities\n",
        "  # - reduction='none': compute loss for each element, don't reduce yet\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "  # Compute the per-token loss\n",
        "  loss = loss_object(label, pred)\n",
        "\n",
        "  # Cast the mask to the same dtype as loss (float32)\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "\n",
        "  # Apply the mask: zero out loss for padding tokens\n",
        "  loss *= mask\n",
        "\n",
        "  # Compute the mean loss over non-padding tokens\n",
        "  loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "  # Get the predicted token IDs by taking argmax over the last axis (vocab)\n",
        "  pred = tf.argmax(pred, axis=2)\n",
        "\n",
        "  # Cast label to the same dtype as pred for comparison\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "\n",
        "  # Compare predicted tokens to true labels\n",
        "  match = label == pred\n",
        "\n",
        "  # Create a mask to ignore padding tokens (assumed to be 0)\n",
        "  mask = label != 0\n",
        "\n",
        "  # Only count matches for non-padding tokens\n",
        "  match = match & mask\n",
        "\n",
        "  # Cast match and mask to float for averaging\n",
        "  match = tf.cast(match, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "\n",
        "  # Compute accuracy as the mean over non-padding tokens\n",
        "  return tf.reduce_sum(match) / tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYEasEOsdn5W"
      },
      "source": [
        "### Train the model üèãÔ∏è‚Äç‚ôÇÔ∏èü§ñ‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk8vwuN24hafK"
      },
      "source": [
        "With all the components ready, configure the training procedure using `model.compile`, and then run it with `model.fit`: üèãÔ∏è‚Äç‚ôÇÔ∏èü§ñ‚ú®\n",
        "\n",
        "Note: This takes about an hour to train in Colab. ‚è≥üíª"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Una1v0hDlIsT"
      },
      "outputs": [],
      "source": [
        "# Compile the Transformer model for training.\n",
        "# - loss: masked_loss function, which computes cross-entropy loss while ignoring padding tokens.\n",
        "# - optimizer: Adam optimizer with a custom learning rate schedule (optimizer variable).\n",
        "# - metrics: masked_accuracy function, which computes accuracy while ignoring padding tokens.\n",
        "# This prepares the model for training with model.fit, ensuring that loss and metrics are calculated correctly for padded sequences.\n",
        "transformer.compile(\n",
        "    loss=masked_loss,          # Custom loss function that ignores padding tokens\n",
        "    optimizer=optimizer,       # Adam optimizer with custom learning rate schedule\n",
        "    metrics=[masked_accuracy]  # Custom accuracy metric that ignores padding tokens\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jg35qKvVlctp"
      },
      "outputs": [],
      "source": [
        "# Train the Transformer model using the training and validation datasets.\n",
        "# - transformer.fit: Starts the training process for the model.\n",
        "# - train_batches: The training dataset, batched and prefetched for efficiency.\n",
        "# - epochs=20: Train the model for 20 complete passes through the training data.\n",
        "# - validation_data=val_batches: Use the validation dataset to evaluate the model after each epoch.\n",
        "# During training:\n",
        "#   1. The model receives batches of input and target sequences from train_batches.\n",
        "#   2. For each batch, it computes predictions, calculates the masked loss and accuracy, and updates weights using the optimizer.\n",
        "#   3. After each epoch, the model evaluates its performance on val_batches to monitor generalization and prevent overfitting.\n",
        "# This process continues for the specified number of epochs, saving training history and metrics.\n",
        "transformer.fit(\n",
        "    train_batches,           # Training data: batched (source, target) pairs\n",
        "    steps_per_epoch=20,      # Number of steps to 20 as it's heavy computation and will take a lot of time.\n",
        "    epochs=1,               # Number of training epochs, 1 for elaboration purposes, otherwise make it 20.\n",
        "    validation_data=val_batches  # Validation data for evaluation after each epoch\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxKpqCbzSW6z"
      },
      "source": [
        "## Run inference üöÄü§ñ‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk8vwuN1SafK"
      },
      "source": [
        "You can now test the model by performing a translation. The following steps are used for inference: üåçü§ñ‚ú®\n",
        "\n",
        "* Encode the input sentence using the Portuguese tokenizer (`tokenizers.pt`). This is the encoder input. üß©üî¢\n",
        "* The decoder input is initialized to the `[START]` token. üö¶\n",
        "* Calculate the padding masks and the look ahead masks. üõ°Ô∏èüëÄ\n",
        "* The `decoder` then outputs the predictions by looking at the `encoder output` and its own output (self-attention). üîÑ‚ú®\n",
        "* Concatenate the predicted token to the decoder input and pass it to the decoder. ‚ûïüîÅ\n",
        "* In this approach, the decoder predicts the next token based on the previous tokens it predicted. ‚è©üß†\n",
        "\n",
        "Note: The model is optimized for _efficient training_ and makes a next-token prediction for each token in the output simultaneously. This is redundant during inference, and only the last prediction is used.  This model can be made more efficient for inference if you only calculate the last prediction when running in inference mode (`training=False`). ‚ö°\n",
        "\n",
        "Define the `Translator` class by subclassing `tf.Module`: üèóÔ∏èü§ñ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eY_uXsOhSmbb"
      },
      "outputs": [],
      "source": [
        "class Translator(tf.Module):\n",
        "  def __init__(self, tokenizers, transformer):\n",
        "    # Store the tokenizers and transformer model for use in translation\n",
        "    self.tokenizers = tokenizers\n",
        "    self.transformer = transformer\n",
        "\n",
        "  def __call__(self, sentence, max_length=MAX_TOKENS):\n",
        "    # The input sentence should be a tf.Tensor (Portuguese string)\n",
        "    assert isinstance(sentence, tf.Tensor)\n",
        "    # If the input is a scalar tensor, add a batch dimension\n",
        "    if len(sentence.shape) == 0:\n",
        "      sentence = sentence[tf.newaxis]\n",
        "\n",
        "    # Tokenize the Portuguese sentence and pad to tensor\n",
        "    sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()\n",
        "    encoder_input = sentence  # Encoder input for the transformer\n",
        "\n",
        "    # Prepare the initial decoder input: English [START] token\n",
        "    start_end = self.tokenizers.en.tokenize([''])[0]\n",
        "    start = start_end[0][tf.newaxis]  # [START] token as tensor\n",
        "    end = start_end[1][tf.newaxis]    # [END] token as tensor\n",
        "\n",
        "    # Use tf.TensorArray for dynamic sequence generation in the loop\n",
        "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "    output_array = output_array.write(0, start)  # Write [START] token at position 0\n",
        "\n",
        "    # Loop to generate each token in the output sequence\n",
        "    for i in tf.range(max_length):\n",
        "      # Stack and transpose output_array to shape (batch, seq_len)\n",
        "      output = tf.transpose(output_array.stack())\n",
        "      # Run the transformer to get predictions for the next token\n",
        "      predictions = self.transformer([encoder_input, output], training=False)\n",
        "      # Select the logits for the last generated token position\n",
        "      predictions = predictions[:, -1:, :]  # Shape: (batch_size, 1, vocab_size)\n",
        "      # Get the predicted token ID (highest probability)\n",
        "      predicted_id = tf.argmax(predictions, axis=-1)\n",
        "      # Append the predicted token to the output sequence\n",
        "      output_array = output_array.write(i+1, predicted_id[0])\n",
        "      # If the predicted token is [END], stop generation\n",
        "      if predicted_id == end:\n",
        "        break\n",
        "\n",
        "    # Final output sequence: transpose to shape (batch, tokens)\n",
        "    output = tf.transpose(output_array.stack())\n",
        "    # Detokenize to get the translated English text\n",
        "    text = tokenizers.en.detokenize(output)[0]  # Shape: ()\n",
        "    # Lookup token strings for the output token IDs\n",
        "    tokens = tokenizers.en.lookup(output)[0]\n",
        "\n",
        "    # Attention weights: recalculate for the final output sequence\n",
        "    # (tf.function tracing prevents direct access inside the loop)\n",
        "    self.transformer([encoder_input, output[:,:-1]], training=False)\n",
        "    attention_weights = self.transformer.decoder.last_attn_scores\n",
        "\n",
        "    # Return the translated text, token strings, and attention weights\n",
        "    return text, tokens, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ3o-65iS6CN"
      },
      "source": [
        "Note: This function uses an unrolled loop, not a dynamic loop. It generates `MAX_TOKENS` on every call. Refer to the [NMT with attention](nmt_with_attention.ipynb) tutorial for an example implementation with a dynamic loop, which can be much more efficient. ‚ö°ü§ñ‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeUJafisS435"
      },
      "source": [
        "Create an instance of this `Translator` class, and try it out a few times: üåçü§ñ‚ú®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NjbvpHUTEia"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the Translator class for inference.\n",
        "# - tokenizers: Contains the Portuguese and English tokenizers for encoding/decoding text.\n",
        "# - transformer: The trained Transformer model for translation.\n",
        "# The Translator class wraps the model and tokenizers, providing a convenient interface\n",
        "# for translating Portuguese sentences to English using the trained Transformer.\n",
        "translator = Translator(tokenizers, transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfHSRdejTFsC"
      },
      "outputs": [],
      "source": [
        "def print_translation(sentence, tokens, ground_truth):\n",
        "  # Print the input sentence (Portuguese or source language)\n",
        "  print(f'{\"Input:\":15s}: {sentence}')\n",
        "\n",
        "  # Print the predicted translation (English or target language)\n",
        "  # - tokens: RaggedTensor containing token strings for the predicted output\n",
        "  # - tokens.numpy(): Converts the RaggedTensor to a NumPy array of bytes\n",
        "  # - .decode(\"utf-8\"): Decodes the byte string to a regular Python string\n",
        "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
        "\n",
        "  # Print the ground truth translation (reference target sentence)\n",
        "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buUeDo58TIoD"
      },
      "source": [
        "Example 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9CEm4cuTGtw"
      },
      "outputs": [],
      "source": [
        "# Define the input sentence in Portuguese and its ground truth English translation.\n",
        "sentence = 'este √© um problema que temos que resolver.'\n",
        "ground_truth = 'this is a problem we have to solve .'\n",
        "\n",
        "# Use the Translator instance to translate the Portuguese sentence to English.\n",
        "# - translator: An instance of the Translator class, which wraps the tokenizers and trained Transformer model.\n",
        "# - tf.constant(sentence): Converts the input sentence to a TensorFlow tensor, as required by the Translator.\n",
        "# - translated_text: The translated English sentence as a string.\n",
        "# - translated_tokens: The tokenized output sequence (tokens) for the translated sentence.\n",
        "# - attention_weights: The cross-attention weights from the Transformer, showing how each output token attends to the input tokens.\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "\n",
        "# Print the input sentence, the predicted translation, and the ground truth translation.\n",
        "# - print_translation: Utility function to display the input, prediction, and reference translation.\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfJrFBZ6TJxc"
      },
      "source": [
        "Example 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elmz_Ly7THuJ"
      },
      "outputs": [],
      "source": [
        "# Define the input sentence in Portuguese and its ground truth English translation.\n",
        "sentence = 'os meus vizinhos ouviram sobre esta ideia.'\n",
        "ground_truth = 'and my neighboring homes heard about this idea .'\n",
        "\n",
        "# Use the Translator instance to translate the Portuguese sentence to English.\n",
        "# - translator: An instance of the Translator class, which wraps the tokenizers and trained Transformer model.\n",
        "# - tf.constant(sentence): Converts the input sentence to a TensorFlow tensor, as required by the Translator.\n",
        "# - translated_text: The translated English sentence as a string.\n",
        "# - translated_tokens: The tokenized output sequence (tokens) for the translated sentence.\n",
        "# - attention_weights: The cross-attention weights from the Transformer, showing how each output token attends to the input tokens.\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "\n",
        "# Print the input sentence, the predicted translation, and the ground truth translation.\n",
        "# - print_translation: Utility function to display the input, prediction, and reference translation.\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY7NfEjrTOCr"
      },
      "source": [
        "Example 3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmmtPo3vTOwj"
      },
      "outputs": [],
      "source": [
        "# Define the input sentence in Portuguese and its ground truth English translation.\n",
        "sentence = 'vou ent√£o muito rapidamente partilhar convosco algumas hist√≥rias de algumas coisas m√°gicas que aconteceram.'\n",
        "ground_truth = \"so i'll just share with you some stories very quickly of some magical things that have happened.\"\n",
        "\n",
        "# Use the Translator instance to translate the Portuguese sentence to English.\n",
        "# - translator: An instance of the Translator class, which wraps the tokenizers and trained Transformer model.\n",
        "# - tf.constant(sentence): Converts the input sentence to a TensorFlow tensor, as required by the Translator.\n",
        "# - translated_text: The translated English sentence as a string.\n",
        "# - translated_tokens: The tokenized output sequence (tokens) for the translated sentence.\n",
        "# - attention_weights: The cross-attention weights from the Transformer, showing how each output token attends to the input tokens.\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "\n",
        "# Print the input sentence, the predicted translation, and the ground truth translation.\n",
        "# - print_translation: Utility function to display the input, prediction, and reference translation.\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB_03k0kTQLb"
      },
      "source": [
        "## Create attention plots üéØ‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miZXl9i-TSs6"
      },
      "source": [
        "The `Translator` class you created in the previous section returns a dictionary of attention heatmaps you can use to visualize the internal working of the model. üéØ‚ú®\n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3m2wcNLTU8K"
      },
      "outputs": [],
      "source": [
        "# Define the input sentence in Portuguese and its ground truth English translation.\n",
        "sentence = 'este √© o primeiro livro que eu fiz.'\n",
        "ground_truth = \"this is the first book i've ever done.\"\n",
        "\n",
        "# Use the Translator instance to translate the Portuguese sentence to English.\n",
        "# - translator: An instance of the Translator class, which wraps the tokenizers and trained Transformer model.\n",
        "# - tf.constant(sentence): Converts the input sentence to a TensorFlow tensor, as required by the Translator.\n",
        "# - translated_text: The translated English sentence as a string.\n",
        "# - translated_tokens: The tokenized output sequence (tokens) for the translated sentence.\n",
        "# - attention_weights: The cross-attention weights from the Transformer, showing how each output token attends to the input tokens.\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "\n",
        "# Print the input sentence, the predicted translation, and the ground truth translation.\n",
        "# - print_translation: Utility function to display the input, prediction, and reference translation.\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rhE_LW7TZ40"
      },
      "source": [
        "Create a function that plots the attention when a token is generated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKlxYO0JTXzD"
      },
      "outputs": [],
      "source": [
        "def plot_attention_head(in_tokens, translated_tokens, attention):\n",
        "  # Remove the <START> token from the translated output tokens for plotting.\n",
        "  translated_tokens = translated_tokens[1:]\n",
        "\n",
        "  # Get the current matplotlib axis for plotting.\n",
        "  ax = plt.gca()\n",
        "  # Display the attention matrix as a heatmap.\n",
        "  ax.matshow(attention)\n",
        "  # Set the x-axis ticks to match the number of input tokens.\n",
        "  ax.set_xticks(range(len(in_tokens)))\n",
        "  # Set the y-axis ticks to match the number of translated tokens (excluding <START>).\n",
        "  ax.set_yticks(range(len(translated_tokens)))\n",
        "\n",
        "  # Decode the input tokens from bytes to strings for labeling the x-axis.\n",
        "  labels = [label.decode('utf-8') for label in in_tokens.numpy()]\n",
        "  ax.set_xticklabels(labels, rotation=90)  # Rotate labels for readability.\n",
        "\n",
        "  # Decode the translated tokens from bytes to strings for labeling the y-axis.\n",
        "  labels = [label.decode('utf-8') for label in translated_tokens.numpy()]\n",
        "  ax.set_yticklabels(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI4YWU2uXDeW"
      },
      "outputs": [],
      "source": [
        "head = 0  # Select which attention head to visualize (0-based index)\n",
        "\n",
        "# The attention_weights tensor has shape (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "# - batch_size: Number of sequences in the batch (usually 1 for inference/visualization)\n",
        "# - num_heads: Number of attention heads in the decoder\n",
        "# - seq_len_q: Length of the query sequence (target/output tokens)\n",
        "# - seq_len_k: Length of the key sequence (source/input tokens)\n",
        "\n",
        "# Remove the batch dimension since we're visualizing a single example\n",
        "attention_heads = tf.squeeze(attention_weights, 0)  # Shape: (num_heads, seq_len_q, seq_len_k)\n",
        "\n",
        "# Select the attention matrix for the desired head\n",
        "attention = attention_heads[head]  # Shape: (seq_len_q, seq_len_k)\n",
        "\n",
        "# Print the shape of the selected attention matrix for verification\n",
        "attention.shape  # Should be (seq_len_q, seq_len_k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "facNouzOXMSu"
      },
      "source": [
        "These are the input (Portuguese) tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMEpyioWTmSN"
      },
      "outputs": [],
      "source": [
        "# Convert the input sentence (Portuguese) to a tensor with batch dimension.\n",
        "in_tokens = tf.convert_to_tensor([sentence])\n",
        "\n",
        "# Tokenize the input sentence using the Portuguese tokenizer.\n",
        "# This converts the string to a sequence of token IDs.\n",
        "in_tokens = tokenizers.pt.tokenize(in_tokens).to_tensor()\n",
        "\n",
        "# Lookup the string representation of each token ID.\n",
        "# This converts the token IDs back to their corresponding token strings.\n",
        "in_tokens = tokenizers.pt.lookup(in_tokens)[0]\n",
        "\n",
        "# Display the list of token strings for the input sentence.\n",
        "in_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLg9HTKCXPKz"
      },
      "source": [
        "And these are the output (English translation) tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzvIo5uYTnHG"
      },
      "outputs": [],
      "source": [
        "translated_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrNh47D1ToBD"
      },
      "outputs": [],
      "source": [
        "# Plot the attention heatmap for a single attention head.\n",
        "# This function visualizes how each output token attends to each input token during translation.\n",
        "# Arguments:\n",
        "#   in_tokens: Tensor of input (source language) tokens as strings.\n",
        "#   translated_tokens: Tensor of output (target language) tokens as strings.\n",
        "#   attention: 2D attention matrix (shape: [output_tokens, input_tokens]) for a single attention head.\n",
        "\n",
        "plot_attention_head(in_tokens, translated_tokens, attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMZr-rI_TrGh"
      },
      "outputs": [],
      "source": [
        "def plot_attention_weights(sentence, translated_tokens, attention_heads):\n",
        "  # Convert the input sentence (string) to a tensor with batch dimension\n",
        "  in_tokens = tf.convert_to_tensor([sentence])\n",
        "  # Tokenize the input sentence using the Portuguese tokenizer and pad to tensor\n",
        "  in_tokens = tokenizers.pt.tokenize(in_tokens).to_tensor()\n",
        "  # Lookup the string representation of each token ID for display\n",
        "  in_tokens = tokenizers.pt.lookup(in_tokens)[0]\n",
        "\n",
        "  # Create a matplotlib figure with a specified size to hold all attention head plots\n",
        "  fig = plt.figure(figsize=(16, 8))\n",
        "\n",
        "  # Iterate over each attention head and its index\n",
        "  for h, head in enumerate(attention_heads):\n",
        "    # Add a subplot for each attention head (2 rows, 4 columns, index h+1)\n",
        "    ax = fig.add_subplot(2, 4, h+1)\n",
        "\n",
        "    # Plot the attention heatmap for this head using the helper function\n",
        "    plot_attention_head(in_tokens, translated_tokens, head)\n",
        "\n",
        "    # Label the x-axis with the head number for clarity\n",
        "    ax.set_xlabel(f'Head {h+1}')\n",
        "\n",
        "  # Adjust subplot layout to prevent overlap and display the figure\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBMujUb1Tr4C"
      },
      "outputs": [],
      "source": [
        "# Plot the attention weights for all attention heads for a given input sentence and its translation.\n",
        "# Arguments:\n",
        "#   sentence: The input sentence (string) in the source language (Portuguese).\n",
        "#   translated_tokens: The output tokens (tensor) from the translation (target language, English).\n",
        "#   attention_weights[0]: The attention weights for the first example in the batch.\n",
        "#     - Shape: (num_heads, output_tokens, input_tokens)\n",
        "#     - Each head's attention matrix shows how each output token attends to each input token.\n",
        "\n",
        "plot_attention_weights(\n",
        "    sentence,            # The input sentence to be translated and visualized.\n",
        "    translated_tokens,   # The tokens of the translated output sentence.\n",
        "    attention_weights[0] # The attention weights for the first example (all heads).\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N5S5IptTtHI"
      },
      "source": [
        "The model can handle unfamiliar words. Neither `'triceratops'` nor `'encyclop√©dia'` are in the input dataset, and the model attempts to transliterate them even without a shared vocabulary. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0-5gjfWT0CS"
      },
      "outputs": [],
      "source": [
        "# Define a Portuguese input sentence and its English ground truth translation.\n",
        "sentence = 'Eu li sobre triceratops na enciclop√©dia.'\n",
        "ground_truth = 'I read about triceratops in the encyclopedia.'\n",
        "\n",
        "# Use the Translator instance to translate the Portuguese sentence to English.\n",
        "# - translator: An instance of the Translator class, which wraps the tokenizers and trained Transformer model.\n",
        "# - tf.constant(sentence): Converts the input sentence to a TensorFlow tensor, as required by the Translator.\n",
        "# - translated_text: The translated English sentence as a string.\n",
        "# - translated_tokens: The tokenized output sequence (tokens) for the translated sentence.\n",
        "# - attention_weights: The cross-attention weights from the Transformer, showing how each output token attends to the input tokens.\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "\n",
        "# Print the input sentence, the predicted translation, and the ground truth translation.\n",
        "# - print_translation: Utility function to display the input, prediction, and reference translation.\n",
        "print_translation(sentence, translated_text, ground_truth)\n",
        "\n",
        "# Visualize the attention weights for all heads for this translation.\n",
        "# - plot_attention_weights: Plots the attention heatmaps for each attention head.\n",
        "# - sentence: The input sentence (Portuguese).\n",
        "# - translated_tokens: The output tokens (English translation).\n",
        "# - attention_weights[0]: The attention weights for the first example in the batch (all heads).\n",
        "plot_attention_weights(sentence, translated_tokens, attention_weights[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zz4uIDbT1OU"
      },
      "source": [
        "## Export the model üì¶‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zunHPJJzT4Cz"
      },
      "source": [
        "You have tested the model and the inference is working. Next, you can export it as a `tf.saved_model`. To learn about saving and loading a model in the SavedModel format, use [this guide](https://www.tensorflow.org/guide/saved_model). üì¶‚ú®\n",
        "\n",
        "Create a class called `ExportTranslator` by subclassing the `tf.Module` subclass with a `tf.function` on the `__call__` method: ü§ñüìù"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZhv5h4AT_n5"
      },
      "outputs": [],
      "source": [
        "class ExportTranslator(tf.Module):\n",
        "  def __init__(self, translator):\n",
        "    # Store the Translator instance, which wraps the tokenizers and trained Transformer model.\n",
        "    self.translator = translator\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
        "  def __call__(self, sentence):\n",
        "    # Run the translation for the input sentence using the Translator.\n",
        "    # - sentence: A scalar tf.Tensor of dtype string (the input sentence to translate).\n",
        "    # - max_length: Maximum number of tokens to generate in the output sequence.\n",
        "    # The Translator returns:\n",
        "    #   - result: The translated sentence as a string.\n",
        "    #   - tokens: The tokenized output sequence (tokens) for the translated sentence.\n",
        "    #   - attention_weights: The cross-attention weights from the Transformer.\n",
        "    (result,\n",
        "     tokens,\n",
        "     attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)\n",
        "\n",
        "    # Return only the translated sentence (result).\n",
        "    # This is the output of the exported model for inference.\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wad7lUtPUAnf"
      },
      "source": [
        "In the above `tf.function` only the output sentence is returned. Thanks to the [non-strict execution](https://tensorflow.org/guide/intro_to_graphs) in `tf.function` any unnecessary values are never computed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7KJEFWI5v84"
      },
      "source": [
        "Wrap `translator` in the newly created `ExportTranslator`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wm1_eRPvUCUm"
      },
      "outputs": [],
      "source": [
        "# Wrap the existing Translator instance in the ExportTranslator class for exporting as a SavedModel.\n",
        "# - ExportTranslator: A tf.Module subclass that provides a tf.function for inference.\n",
        "#   This allows you to export the translation model for serving or deployment.\n",
        "# - translator: The Translator instance, which contains the tokenizers and trained Transformer model.\n",
        "# After this assignment, 'translator' refers to the ExportTranslator instance,\n",
        "# which can be used for inference and exporting the model.\n",
        "translator = ExportTranslator(translator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VPH4T5XUDnc"
      },
      "source": [
        "Since the model is decoding the predictions using `tf.argmax` the predictions are deterministic. The original model and one reloaded from its `SavedModel` should give identical predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GITRCiAYUE5w"
      },
      "outputs": [],
      "source": [
        "# Translate a Portuguese sentence to English using the exported Translator model.\n",
        "# - translator: An instance of ExportTranslator, which wraps the trained Translator and Transformer model.\n",
        "# - The input is a Portuguese sentence as a string.\n",
        "# - The output is a TensorFlow string tensor containing the translated English sentence.\n",
        "# - .numpy(): Converts the output tensor to a NumPy array (or Python string) for display.\n",
        "translator('este √© o primeiro livro que eu fiz.').numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_v--e1XmUFw3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Directory where the model will be saved\n",
        "SAVE_DIR = \"artifacts\"\n",
        "\n",
        "# Name for the exported model\n",
        "model_name = \"translator\"\n",
        "\n",
        "# Full path to save the model (artifacts/translator)\n",
        "MODEL_PATH = os.path.join(SAVE_DIR, model_name)\n",
        "\n",
        "# Export the translator model as a TensorFlow SavedModel.\n",
        "# - translator: The ExportTranslator instance, which wraps the trained Translator and Transformer model.\n",
        "# - export_dir: The directory path where the SavedModel will be stored.\n",
        "# This will create the directory and save all necessary files for loading and serving the model later.\n",
        "tf.saved_model.save(translator, export_dir=MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KJSQEzlUGo-"
      },
      "outputs": [],
      "source": [
        "# Load the exported TensorFlow SavedModel from disk.\n",
        "# - MODEL_PATH: The directory path where the SavedModel was saved (e.g., \"artifacts/translator\").\n",
        "# - tf.saved_model.load: Loads the model and returns a callable object for inference.\n",
        "#   This object can be used to perform translation using the exported model.\n",
        "#   The loaded model will have the same behavior as the original ExportTranslator instance.\n",
        "reloaded = tf.saved_model.load(MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIVpKWBNUHhr"
      },
      "outputs": [],
      "source": [
        "# Use the reloaded SavedModel to translate a Portuguese sentence to English.\n",
        "# - reloaded: This is the loaded TensorFlow SavedModel, which wraps the ExportTranslator module.\n",
        "# - The input is a Portuguese sentence as a string.\n",
        "# - The output is a TensorFlow string tensor containing the translated English sentence.\n",
        "# - .numpy(): Converts the output tensor to a NumPy array (or Python string) for display.\n",
        "# This demonstrates that the exported and reloaded model produces the same deterministic output as the original model.\n",
        "reloaded('este √© o primeiro livro que eu fiz.').numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri2i6cTxUI00"
      },
      "source": [
        "## Conclusion üéâü§ñ‚ú®\n",
        "\n",
        "In this tutorial you learned about:\n",
        "\n",
        "* The Transformers and their significance in machine learning üöÄ\n",
        "* Attention, self-attention and multi-head attention üéØ\n",
        "* Positional encoding with embeddings üß©\n",
        "* The encoder-decoder architecture of the original Transformer üèóÔ∏è\n",
        "* Masking in self-attention üõ°Ô∏è\n",
        "* How to put it all together to translate text üåç\n",
        "\n",
        "The downsides of this architecture are:\n",
        "\n",
        "- For a time-series, the output for a time-step is calculated from the *entire history* instead of only the inputs and current hidden-state. This _may_ be less efficient. ‚è≥\n",
        "- If the input has a temporal/spatial relationship, like text or images, some positional encoding must be added or the model will effectively see a bag of words. üóÇÔ∏è\n",
        "\n",
        "If you want to practice, there are many things you could try with it. For example:\n",
        "\n",
        "* Use a different dataset to train the Transformer. üìö\n",
        "* Create the \"Base Transformer\" or \"Transformer XL\" configurations from the original paper by changing the hyperparameters. üõ†Ô∏è\n",
        "* Use the layers defined here to create an implementation of [BERT](https://arxiv.org/abs/1810.04805) üêª\n",
        "* Use Beam search to get better predictions. üîé\n",
        "\n",
        "There are a wide variety of Transformer-based models, many of which improve upon the 2017 version of the original Transformer with encoder-decoder, encoder-only and decoder-only architectures.\n",
        "\n",
        "Some of these models are covered in the following research publications:\n",
        "\n",
        "* [\"Efficient Transformers: a survey\"](https://arxiv.org/abs/2009.06732) (Tay et al., 2022) ‚ö°\n",
        "* [\"Formal algorithms for Transformers\"](https://arxiv.org/abs/2207.09238) (Phuong and Hutter, 2022). üìê\n",
        "* [T5 (\"Exploring the limits of transfer learning with a unified text-to-text Transformer\")](https://arxiv.org/abs/1910.10683) (Raffel et al., 2019) üî§\n",
        "\n",
        "You can learn more about other models in the following Google blog posts:\n",
        "\n",
        "* [PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html). üå¥\n",
        "* [LaMDA](https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html) üó£Ô∏è\n",
        "* [MUM](https://blog.google/products/search/introducing-mum/) ü§π\n",
        "* [Reformer](https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html) üîÅ\n",
        "* [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) üêª\n",
        "\n",
        "If you're interested in studying how attention-based models have been applied in tasks outside of natural language processing, check out the following resources:\n",
        "\n",
        "- Vision Transformer (ViT): [Transformers for image recognition at scale](https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html) üñºÔ∏è\n",
        "- [Multi-task multitrack music transcription (MT3)](https://magenta.tensorflow.org/transcription-with-transformers) with a Transformer üéµ\n",
        "- [Code generation with AlphaCode](https://www.deepmind.com/blog/competitive-programming-with-alphacode) üíª\n",
        "- [Reinforcement learning with multi-game decision Transformers](https://ai.googleblog.com/2022/07/training-generalist-agents-with-multi.html) üéÆ\n",
        "- [Protein structure prediction with AlphaFold](https://www.nature.com/articles/s41586-021-03819-2) üß¨\n",
        "- [OptFormer: Towards universal hyperparameter optimization with Transformers](http://ai.googleblog.com/2022/08/optformer-towards-universal.html) ‚öôÔ∏è\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}